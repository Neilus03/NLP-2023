{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neilus03/NLP-2023/blob/main/%5BGIA%5D_%5BNLP%5D_Seq_Labeling_and_NER_with_HMM_and_CRF_DANI_%26_NEIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Daniel Vidal, NIU: 1634599\n",
        "# Neil de la Fuente, NIU: 1630223"
      ],
      "metadata": {
        "id": "EPYXKjUJPLVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Sequence Labeling With CRF and HMM - NER\n",
        "\n",
        "The extraction of relevant information from historical handwritten document collections is one of the key steps in order to make these manuscripts available for access and searches. In this context, instead of a pure transcription, the objective is to move towards document understanding. Concretely, the aim is to detect the named entities and assign each of them a semantic category, such as family names, places, occupations, etc.\n",
        "\n",
        "\n",
        "A typical application scenario of named entity recognition is demographic documents, since they contain people's names, birthplaces, occupations, etc. In this scenario, the extraction of the key contents and its storage in databases allows the access to their contents and envision innovative services based in genealogical, social or demographic searches.\n",
        "\n",
        "<p style = 'text-align: center'>\n",
        "<img src = \"http://dag.cvc.uab.es/wp-content/uploads/2016/07/esposalla_detall.jpg\">\n",
        "</p>\n",
        "\n",
        "For further doubts and questions, refer to oriol.ramos@uab.cat and alicia.fornes@uab.cat.\n",
        "\n",
        "Usage of Google Colab is not mandatory, but highly recommended as most of the behaviors are expected for a Linux VM with IPython bindings.\n",
        "\n",
        "## First, we will install the unmet dependencies.\n",
        "\n",
        "This will download some packages and the required data, it may take a while."
      ],
      "metadata": {
        "id": "iXYMGLM2UTG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R8Rcfr3oUAms",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title \n",
        "from IPython.display import clear_output\n",
        "\n",
        "!git clone https://github.com/EauDeData/nlp-resources\n",
        "!cp -r nlp-resources/ resources/\n",
        "!rm -rf nlp-resources/\n",
        "\n",
        "\n",
        "!pip install nltk \n",
        "!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite\n",
        "clear_output()\n",
        "\n",
        "from typing import * \n",
        "\n",
        "from itertools import chain\n",
        "import nltk\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "import pycrfsuite as crfs\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from resources.data.dataloaders import EsposallesTextDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Curation\n",
        "Loading the dataset - From here you could re-use your previous work."
      ],
      "metadata": {
        "id": "2LbPYoxkYGUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "train_loader = EsposallesTextDataset('resources/data/esposalles/') \n",
        "test_loader = copy.deepcopy(train_loader)\n",
        "test_loader.test()"
      ],
      "metadata": {
        "id": "NRLKbU-4UQ8O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of data from each loader:\n",
        "> Format string: ```word```:```label```\n"
      ],
      "metadata": {
        "id": "JzOLIKMGUGAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([f\"{x}:{y}\" for x,y in zip(*train_loader[0])])\n",
        "print([f\"{x}:{y}\" for x,y in zip(*test_loader[0])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6nElJ2-ZCJe",
        "outputId": "4ded027f-e824-442e-de92-fdbb0a040668"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dilluns:other', 'a:other', '5:other', 'rebere:other', 'de:other', 'Hyacinto:name', 'Boneu:surname', 'hortola:occupation', 'de:other', 'Bara:location', 'fill:other', 'de:other', 'Juan:name', 'Boneu:surname', 'parayre:occupation', 'defunct:other', 'y:other', 'de:other', 'Maria:name', 'ab:other', 'Anna:name', 'donsella:state', 'filla:other', 'de:other', 't:name', 'Cases:surname', 'pages:occupation', 'de:other', 'Bara:location', 'defunct:other', 'y:other', 'de:other', 'Peyrona:name']\n",
            "['Divendres:other', 'a:other', '18:other', 'rebere:other', 'de:other', 'Juan:name', 'Torres:surname', 'pages:occupation', 'habitant:other', 'en:other', 'Sabadell:location', 'fill:other', 'de:other', 'Bernat:name', 'Torres:surname', 'pages:occupation', 'de:other', 'Moya:location', 'bisbat:location', 'de:location', 'Vich:location', 'y:other', 'de:other', 'Antiga:name', 'defucts:other', 'ab:other', 'Margarida:name', 'donsella:state', 'filla:other', 'de:other', 'Juan:name', 'Argemir:surname', 'pages:occupation', 'de:other', 'Sabadell:location', 'y:other', 'de:other', 'Aldonsa:name', 'defuncts:other']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is correctly downloaded you will see two different samples above, and both tests passed below."
      ],
      "metadata": {
        "id": "PXOg5M3-lg5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "# Dataset ckeck\n",
        "\n",
        "for idx in range(len(train_loader)):\n",
        "\n",
        "  x, y = train_loader[idx]\n",
        "  if len(x) != len(y): \n",
        "    print('train_set test not passed')\n",
        "    break\n",
        "\n",
        "else: print('train_set test passed')\n",
        "\n",
        "for idx in range(len(test_loader)):\n",
        "\n",
        "  x, y = test_loader[idx]\n",
        "  if len(x) != len(y):\n",
        "    print('test_set test not passed')\n",
        "    break\n",
        "\n",
        "else: print('test_set test passed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7_mFDqOkcnW",
        "outputId": "ab87da4d-a004-4e5f-cc60-ad810bf18217"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_set test passed\n",
            "test_set test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since most of the computation won't be done with strings, the following function will create a Look Up Table (LUT) that transforms string tokens into ```int``` tokens. "
      ],
      "metadata": {
        "id": "nqp5OuN8YTwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tokens_lut(train_dataset) -> Dict:\n",
        "    '''\n",
        "    Input:\n",
        "        train_dataset: Training dataset. \n",
        "\n",
        "        Don't tokenize test_set as later on,\n",
        "        we will be considering out-of-vocabulary words as <unk> tokens.\n",
        "\n",
        "        NOTE: Tokens MUST be lowered (.lower()) before considering them. \n",
        "\n",
        "    Ouput:\n",
        "        LUT[Dict]: {\n",
        "        word1: 0,\n",
        "        word2: 1,\n",
        "            ...\n",
        "        wordn: n - 1\n",
        "        }\n",
        "\n",
        "    '''\n",
        "    token_to_id = {}\n",
        "    current_id = 0\n",
        "    \n",
        "    for i in range(len(train_dataset)):\n",
        "        for word, label in zip(*train_dataset[i]):\n",
        "            token = word.lower()  # Lowercase the token\n",
        "\n",
        "            if token not in token_to_id:\n",
        "                token_to_id[token] = current_id # Each new token is saved with the corresponding ID\n",
        "                current_id += 1\n",
        "\n",
        "    return token_to_id\n",
        "\n",
        "LUT = create_tokens_lut(train_loader)\n",
        "LUT"
      ],
      "metadata": {
        "id": "rt-8QpTzYx2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad444dd-1aa5-4b16-f1f7-9f3a021bff29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dilluns': 0,\n",
              " 'a': 1,\n",
              " '5': 2,\n",
              " 'rebere': 3,\n",
              " 'de': 4,\n",
              " 'hyacinto': 5,\n",
              " 'boneu': 6,\n",
              " 'hortola': 7,\n",
              " 'bara': 8,\n",
              " 'fill': 9,\n",
              " 'juan': 10,\n",
              " 'parayre': 11,\n",
              " 'defunct': 12,\n",
              " 'y': 13,\n",
              " 'maria': 14,\n",
              " 'ab': 15,\n",
              " 'anna': 16,\n",
              " 'donsella': 17,\n",
              " 'filla': 18,\n",
              " 't': 19,\n",
              " 'cases': 20,\n",
              " 'pages': 21,\n",
              " 'peyrona': 22,\n",
              " 'dit': 23,\n",
              " 'dia': 24,\n",
              " 'bernat': 25,\n",
              " 'call': 26,\n",
              " 'st': 27,\n",
              " 'esteva': 28,\n",
              " 'palau': 29,\n",
              " 'tordera': 30,\n",
              " 'guille': 31,\n",
              " 'catherina': 32,\n",
              " 'guillem': 33,\n",
              " 'boix': 34,\n",
              " 'la': 35,\n",
              " 'mora': 36,\n",
              " 'bisbat': 37,\n",
              " 'vich': 38,\n",
              " 'margarida': 39,\n",
              " 'jua': 40,\n",
              " 'perega': 41,\n",
              " 'comissari': 42,\n",
              " 'real': 43,\n",
              " 'habitant': 44,\n",
              " 'en': 45,\n",
              " 'viudo': 46,\n",
              " 'gratia': 47,\n",
              " 'carbonell': 48,\n",
              " 'negociant': 49,\n",
              " 'defuncts': 50,\n",
              " 'dijous': 51,\n",
              " '10': 52,\n",
              " 'montserrat': 53,\n",
              " 'gibert': 54,\n",
              " 'treballador': 55,\n",
              " 'traginer': 56,\n",
              " 'mar': 57,\n",
              " 'janota': 58,\n",
              " 'magdalena': 59,\n",
              " 'sabater': 60,\n",
              " 'dissapte': 61,\n",
              " '26': 62,\n",
              " 'jaume': 63,\n",
              " '#': 64,\n",
              " 'oller': 65,\n",
              " 'andreu': 66,\n",
              " 'pa': 67,\n",
              " 'lomar': 68,\n",
              " 'eularia': 69,\n",
              " 'pere': 70,\n",
              " 'llobateres': 71,\n",
              " 'ripollet': 72,\n",
              " 'maryanna': 73,\n",
              " 'dimecres': 74,\n",
              " '13': 75,\n",
              " '#donare': 76,\n",
              " 'llicentia': 77,\n",
              " 'antoni': 78,\n",
              " 'dauban': 79,\n",
              " 'del': 80,\n",
              " 'regne': 81,\n",
              " 'frança': 82,\n",
              " 'lo': 83,\n",
              " 'prat': 84,\n",
              " 'saldonia': 85,\n",
              " 'miquel': 86,\n",
              " 'sola': 87,\n",
              " 'orista': 88,\n",
              " 'elisabeth': 89,\n",
              " 'defucts': 90,\n",
              " 'diumege': 91,\n",
              " '16': 92,\n",
              " 'claus': 93,\n",
              " 'frances': 94,\n",
              " 'spaser': 95,\n",
              " 'habitat': 96,\n",
              " 'llacuna': 97,\n",
              " 'mag': 98,\n",
              " 'dalena': 99,\n",
              " 'valenti': 100,\n",
              " 'colomer': 101,\n",
              " 'pierola': 102,\n",
              " 'mates': 103,\n",
              " 'dosella': 104,\n",
              " 'lloch': 105,\n",
              " 'gabriel': 106,\n",
              " 'janer': 107,\n",
              " 'aulesa': 108,\n",
              " 'defuncta': 109,\n",
              " 'juana': 110,\n",
              " 'bruguera': 111,\n",
              " 'tots': 112,\n",
              " 'dita': 113,\n",
              " 'parrochia': 114,\n",
              " 'reberem': 115,\n",
              " 'arenas': 116,\n",
              " 'artes': 117,\n",
              " 'an': 118,\n",
              " 'dreu': 119,\n",
              " 'palomar': 120,\n",
              " 'antich': 121,\n",
              " 'angela': 122,\n",
              " 'als': 123,\n",
              " 'vilar': 124,\n",
              " 'francesch': 125,\n",
              " 'regordosa': 126,\n",
              " 'passamaner': 127,\n",
              " 'sarria': 128,\n",
              " 'beneta': 129,\n",
              " 'viuda': 130,\n",
              " 'golfes': 131,\n",
              " '31': 132,\n",
              " 'baldiri': 133,\n",
              " 'livia': 134,\n",
              " 'cornet': 135,\n",
              " 'manresa': 136,\n",
              " 'salvadora': 137,\n",
              " 'llorens': 138,\n",
              " 'moliner': 139,\n",
              " 'fransa': 140,\n",
              " 'iscla': 141,\n",
              " 'clara': 142,\n",
              " 'serra': 143,\n",
              " 'gurb': 144,\n",
              " 'dimars': 145,\n",
              " '12': 146,\n",
              " 'petit': 147,\n",
              " 'olivella': 148,\n",
              " 'sitges': 149,\n",
              " 'magi': 150,\n",
              " 'oliver': 151,\n",
              " 'hierona': 152,\n",
              " 'marti': 153,\n",
              " 'forner': 154,\n",
              " 'çarreal': 155,\n",
              " 'archebisbat': 156,\n",
              " 'tar': 157,\n",
              " 'ragona': 158,\n",
              " 'figue': 159,\n",
              " 'res': 160,\n",
              " 'divendres': 161,\n",
              " '15': 162,\n",
              " 'pasqual': 163,\n",
              " 'corder': 164,\n",
              " 'caldes': 165,\n",
              " 'monbuy': 166,\n",
              " 'cathe': 167,\n",
              " 'rina': 168,\n",
              " 'camp': 169,\n",
              " 'antonia': 170,\n",
              " 'benell': 171,\n",
              " 'calsater': 172,\n",
              " 'agulla': 173,\n",
              " 'phelip': 174,\n",
              " 'ges': 175,\n",
              " 'serdanya': 176,\n",
              " 'urgell': 177,\n",
              " 'pau': 178,\n",
              " 'junqueres': 179,\n",
              " 'eli': 180,\n",
              " 'sabeth': 181,\n",
              " 'bosch': 182,\n",
              " 'llissa': 183,\n",
              " 'demunt': 184,\n",
              " 'juna': 185,\n",
              " '25': 186,\n",
              " 'ramon': 187,\n",
              " 'joan': 188,\n",
              " 'hieronyma': 189,\n",
              " 'paula': 190,\n",
              " 'delvi': 191,\n",
              " 'centelles': 192,\n",
              " 'violant': 193,\n",
              " 'hieronym': 194,\n",
              " 'alella': 195,\n",
              " 'pons': 196,\n",
              " 'joval': 197,\n",
              " 'solsona': 198,\n",
              " 'mori': 199,\n",
              " 'devant': 200,\n",
              " 'barthomeu': 201,\n",
              " 'lacumba': 202,\n",
              " 'pastisser': 203,\n",
              " 'davesa': 204,\n",
              " 'romeu': 205,\n",
              " 'vilaredona': 206,\n",
              " 'botiguer': 207,\n",
              " 'montserrada': 208,\n",
              " 'phi': 209,\n",
              " 'lippa': 210,\n",
              " 'gari': 211,\n",
              " 'matalafer': 212,\n",
              " 'vilanova': 213,\n",
              " 'speransa': 214,\n",
              " 'per': 215,\n",
              " 'mans': 216,\n",
              " 'sr': 217,\n",
              " 'canoge': 218,\n",
              " 'amell': 219,\n",
              " 'vicari': 220,\n",
              " 'general': 221,\n",
              " 'bertran': 222,\n",
              " 'baxador': 223,\n",
              " 'eleonor': 224,\n",
              " 'vinyals': 225,\n",
              " '18': 226,\n",
              " 'roig': 227,\n",
              " 'thomas': 228,\n",
              " 'torroella': 229,\n",
              " 'mariner': 230,\n",
              " 'rafols': 231,\n",
              " 'boter': 232,\n",
              " 'cubelles': 233,\n",
              " 'elienor': 234,\n",
              " 'crespi': 235,\n",
              " 'algutsir': 236,\n",
              " 'extraordimari': 237,\n",
              " 'die': 238,\n",
              " 'ambros': 239,\n",
              " 'samora': 240,\n",
              " 'gelida': 241,\n",
              " 'piloter': 242,\n",
              " 'barca': 243,\n",
              " 'papiol': 244,\n",
              " 'sylvestre': 245,\n",
              " 'valiellas': 246,\n",
              " 'bernabe': 247,\n",
              " 'riera': 248,\n",
              " 'granollers': 249,\n",
              " 'blanquer': 250,\n",
              " 'miquela': 251,\n",
              " 'marianna': 252,\n",
              " 'major': 253,\n",
              " 'sastre': 254,\n",
              " 'vilassar': 255,\n",
              " 'luys': 256,\n",
              " 'soler': 257,\n",
              " 'prats': 258,\n",
              " 'colom': 259,\n",
              " 'pens': 260,\n",
              " 'francisca': 261,\n",
              " 'busquets': 262,\n",
              " 'valldoreig': 263,\n",
              " 'ribes': 264,\n",
              " 'feliu': 265,\n",
              " 'llobregat': 266,\n",
              " '21': 267,\n",
              " 'carles': 268,\n",
              " 'coll': 269,\n",
              " 'marques': 270,\n",
              " 'fuster': 271,\n",
              " 'mataro': 272,\n",
              " 'guiteres': 273,\n",
              " 'carra': 274,\n",
              " 'ter': 275,\n",
              " 'faura': 276,\n",
              " 'badalona': 277,\n",
              " 'simeon': 278,\n",
              " 'roldos': 279,\n",
              " 'vivent': 280,\n",
              " 'masnou': 281,\n",
              " 'collsabadell': 282,\n",
              " 'mas': 283,\n",
              " 'argentona': 284,\n",
              " 'ferrer': 285,\n",
              " 'premia': 286,\n",
              " 'balthesar': 287,\n",
              " 'morera': 288,\n",
              " 'susanna': 289,\n",
              " '27': 290,\n",
              " 'mr': 291,\n",
              " 'raphel': 292,\n",
              " 'candaler': 293,\n",
              " 'cera': 294,\n",
              " 'mathiana': 295,\n",
              " 'arcis': 296,\n",
              " 'planes': 297,\n",
              " 'vedrier': 298,\n",
              " 'sperasa': 299,\n",
              " 'clos': 300,\n",
              " 'doctor': 301,\n",
              " 'drets': 302,\n",
              " 'ciuteda': 303,\n",
              " 'sra': 304,\n",
              " 'raphela': 305,\n",
              " 'burgues': 306,\n",
              " 'puigcerda': 307,\n",
              " 'torres': 308,\n",
              " 'ponsa': 309,\n",
              " 'vicens': 310,\n",
              " 'hortela': 311,\n",
              " 'montells': 312,\n",
              " 'franch': 313,\n",
              " 'corredor': 314,\n",
              " 'orella': 315,\n",
              " 'illa': 316,\n",
              " 'julita': 317,\n",
              " '24': 318,\n",
              " 'dequer': 319,\n",
              " 'basi': 320,\n",
              " 'sadorni': 321,\n",
              " '23': 322,\n",
              " 'aguilar': 323,\n",
              " 'eu': 324,\n",
              " 'laria': 325,\n",
              " 'boxo': 326,\n",
              " 'carrater': 327,\n",
              " 'arus': 328,\n",
              " 'sabadell': 329,\n",
              " 'dionysa': 330,\n",
              " 'do': 331,\n",
              " 'sella': 332,\n",
              " 'pujades': 333,\n",
              " 'vila': 334,\n",
              " '17': 335,\n",
              " 'alzina': 336,\n",
              " 'cardedeu': 337,\n",
              " 'joana': 338,\n",
              " 'joa': 339,\n",
              " 'gori': 340,\n",
              " 'vilamajor': 341,\n",
              " 'arnau': 342,\n",
              " 'locos': 343,\n",
              " 'cotxero': 344,\n",
              " 'serralta': 345,\n",
              " 'sala': 346,\n",
              " 'taya': 347,\n",
              " 'dels': 348,\n",
              " 'damut': 349,\n",
              " 'dits': 350,\n",
              " 'morato': 351,\n",
              " 'gerrer': 352,\n",
              " 'sal': 353,\n",
              " 'vador': 354,\n",
              " 'maryangela': 355,\n",
              " 'jorba': 356,\n",
              " 'manovelles': 357,\n",
              " 'collbato': 358,\n",
              " 'nadal': 359,\n",
              " 'elies': 360,\n",
              " 'texidor': 361,\n",
              " 'lli': 362,\n",
              " 'serinya': 363,\n",
              " 'çavall': 364,\n",
              " 'rexach': 365,\n",
              " 'ramo': 366,\n",
              " 'segui': 367,\n",
              " '30': 368,\n",
              " 'xiol': 369,\n",
              " 'animals': 370,\n",
              " 'fill#': 371,\n",
              " 'dorothea': 372,\n",
              " 'gili': 373,\n",
              " 'catha': 374,\n",
              " 'diumenge': 375,\n",
              " '22': 376,\n",
              " 'semoler': 377,\n",
              " 'mari': 378,\n",
              " 'balcells': 379,\n",
              " '11': 380,\n",
              " 'font': 381,\n",
              " 'terrassa': 382,\n",
              " 'marga': 383,\n",
              " 'rida': 384,\n",
              " 'narcis': 385,\n",
              " 'guitart': 386,\n",
              " 'janot': 387,\n",
              " 'vilardona': 388,\n",
              " 'bellsol': 389,\n",
              " 'vilaro': 390,\n",
              " 'dona': 391,\n",
              " 'comadara': 392,\n",
              " 'ça': 393,\n",
              " 'vall': 394,\n",
              " 'espalter': 395,\n",
              " '6': 396,\n",
              " 'figuerola': 397,\n",
              " 'bal': 398,\n",
              " 'thesar': 399,\n",
              " 'torrellas': 400,\n",
              " 'puig': 401,\n",
              " 'las': 402,\n",
              " 'fexes': 403,\n",
              " 'duran': 404,\n",
              " 'mollet': 405,\n",
              " '7': 406,\n",
              " 'alomar': 407,\n",
              " 'calafat': 408,\n",
              " 'casanoves': 409,\n",
              " 'pescador': 410,\n",
              " 'garriga': 411,\n",
              " 'cabrera': 412,\n",
              " 'piera': 413,\n",
              " 'tudo': 414,\n",
              " 'strada': 415,\n",
              " 'cirurgia': 416,\n",
              " 'perpinya': 417,\n",
              " 'notari': 418,\n",
              " 'parelada': 419,\n",
              " 'telas': 420,\n",
              " 'sebastia': 421,\n",
              " 'guixols': 422,\n",
              " 'girona': 423,\n",
              " 'elisa': 424,\n",
              " 'beth': 425,\n",
              " 'peller': 426,\n",
              " 'tarragona': 427,\n",
              " 'barrera': 428,\n",
              " 'castellbo': 429,\n",
              " 'simon': 430,\n",
              " 'pedros': 431,\n",
              " 'tudela': 432,\n",
              " 'joseph': 433,\n",
              " 'giralt': 434,\n",
              " 'hostaler': 435,\n",
              " 'tremoler': 436,\n",
              " 'vidal': 437,\n",
              " 'pas': 438,\n",
              " 'dols': 439,\n",
              " 'malet': 440,\n",
              " 'robi': 441,\n",
              " 'bis': 442,\n",
              " 'bat': 443,\n",
              " 'comes': 444,\n",
              " 'dorrius': 445,\n",
              " 'eufrahina': 446,\n",
              " 'portas': 447,\n",
              " 'feraut': 448,\n",
              " 'retorsedor': 449,\n",
              " 'llana': 450,\n",
              " 'jonchar': 451,\n",
              " 'çacamp': 452,\n",
              " 'rocha': 453,\n",
              " '28': 454,\n",
              " 'rubi': 455,\n",
              " 'stephania': 456,\n",
              " 'barcelo': 457,\n",
              " '29': 458,\n",
              " 'sayol': 459,\n",
              " 'marcia': 460,\n",
              " 'francesca': 461,\n",
              " 'pros': 462,\n",
              " 'costant': 463,\n",
              " 'agusti': 464,\n",
              " 'flassader': 465,\n",
              " 'vivint': 466,\n",
              " 'eula': 467,\n",
              " 'ria': 468,\n",
              " 'oriol': 469,\n",
              " 'tamarit': 470,\n",
              " 'donsell': 471,\n",
              " 'domiciliat': 472,\n",
              " 'perot': 473,\n",
              " 'raymunda': 474,\n",
              " 'vilagaya': 475,\n",
              " 'corbera': 476,\n",
              " 'isabel': 477,\n",
              " 'guardia': 478,\n",
              " 'argenter': 479,\n",
              " 'madrona': 480,\n",
              " 'martell': 481,\n",
              " 'garbellador': 482,\n",
              " 'ricas': 483,\n",
              " 'blanch': 484,\n",
              " 'camprodo': 485,\n",
              " 'llinyola': 486,\n",
              " 'domenja': 487,\n",
              " 'vilafranca': 488,\n",
              " 'panaders': 489,\n",
              " 'sesoliveres': 490,\n",
              " 'subirats': 491,\n",
              " 'badia': 492,\n",
              " 'iga': 493,\n",
              " 'solinyach': 494,\n",
              " 'sobirats': 495,\n",
              " 'ferriola': 496,\n",
              " 'estella': 497,\n",
              " 'carrovires': 498,\n",
              " 'trullas': 499,\n",
              " 'decavals': 500,\n",
              " 'marser': 501,\n",
              " 'onofre': 502,\n",
              " 'canyet': 503,\n",
              " 'daguer': 504,\n",
              " 'bus': 505,\n",
              " 'quets': 506,\n",
              " 'oliva': 507,\n",
              " 'vaquer': 508,\n",
              " 'jove': 509,\n",
              " 'mestre': 510,\n",
              " 'natural': 511,\n",
              " 'gim': 512,\n",
              " 'rabassa': 513,\n",
              " 'sescomes': 514,\n",
              " 'beneya': 515,\n",
              " 'adroguer': 516,\n",
              " 'habi': 517,\n",
              " 'tant': 518,\n",
              " 'agnes': 519,\n",
              " 'arbos': 520,\n",
              " 'assahonador': 521,\n",
              " 'barrater': 522,\n",
              " 'mariangela': 523,\n",
              " 'torrent': 524,\n",
              " 'ceu': 525,\n",
              " '4': 526,\n",
              " 'massana': 527,\n",
              " 'pontons': 528,\n",
              " 'mitja': 529,\n",
              " 'mercader': 530,\n",
              " 'contijoch': 531,\n",
              " 'comalada': 532,\n",
              " 'cilurgia': 533,\n",
              " '19': 534,\n",
              " 'salvador': 535,\n",
              " 'joli': 536,\n",
              " 'bellver': 537,\n",
              " 'lleyda': 538,\n",
              " 'aliana': 539,\n",
              " 'fetjo': 540,\n",
              " 'cugat': 541,\n",
              " 'vallers': 542,\n",
              " 'moner': 543,\n",
              " 'costa': 544,\n",
              " 'maryana': 545,\n",
              " 'seloni': 546,\n",
              " 'puigmari': 547,\n",
              " 'damia': 548,\n",
              " 'hostal': 549,\n",
              " 'rich': 550,\n",
              " 'vilalba': 551,\n",
              " 'sagimon': 552,\n",
              " 'ferran': 553,\n",
              " 'donare': 554,\n",
              " 'gratis': 555,\n",
              " 'dosseig': 556,\n",
              " 'pia': 557,\n",
              " 'elna': 558,\n",
              " '14': 559,\n",
              " 'benet': 560,\n",
              " 'ifern': 561,\n",
              " 'farrer': 562,\n",
              " 'bigues': 563,\n",
              " 'garida': 564,\n",
              " 'blancafort': 565,\n",
              " 'boger': 566,\n",
              " 'montmany': 567,\n",
              " 'figueres': 568,\n",
              " 'sinter': 569,\n",
              " 'praxedis': 570,\n",
              " 'colell': 571,\n",
              " 'franca': 572,\n",
              " 'calvell': 573,\n",
              " 'sta': 574,\n",
              " 'perpetua': 575,\n",
              " 'ribalta': 576,\n",
              " 'granera': 577,\n",
              " 'march': 578,\n",
              " 'lledo': 579,\n",
              " 'magina': 580,\n",
              " 'ricart': 581,\n",
              " 'matheu': 582,\n",
              " 'marcer': 583,\n",
              " 'bellmut': 584,\n",
              " 'antiga': 585,\n",
              " 'savina': 586,\n",
              " 'gualba': 587,\n",
              " 'mercer': 588,\n",
              " 'victoria': 589,\n",
              " '8': 590,\n",
              " 'mont': 591,\n",
              " 'serrat': 592,\n",
              " 'ninou': 593,\n",
              " 'candia': 594,\n",
              " 'miql': 595,\n",
              " 'riu': 596,\n",
              " 'billes': 597,\n",
              " 'verdaguer': 598,\n",
              " 'llampis': 599,\n",
              " 'llibo': 600,\n",
              " 'monica': 601,\n",
              " 'vels': 602,\n",
              " 'sans': 603,\n",
              " 'masmitja': 604,\n",
              " 'sparaguera': 605,\n",
              " 'cantarell': 606,\n",
              " 'axa': 607,\n",
              " 'barbera': 608,\n",
              " 'rossell': 609,\n",
              " 'arago': 610,\n",
              " 'funct': 611,\n",
              " 'cardalech': 612,\n",
              " 'olivelles': 613,\n",
              " 'campins': 614,\n",
              " 'nicolau': 615,\n",
              " 'sanahuja': 616,\n",
              " 'despi': 617,\n",
              " 'boria': 618,\n",
              " 'mot': 619,\n",
              " 'boy': 620,\n",
              " 'francina': 621,\n",
              " 'alfonso': 622,\n",
              " 'margarit': 623,\n",
              " 'masquefa': 624,\n",
              " 'llavaneres': 625,\n",
              " 'arenys': 626,\n",
              " 'barriga': 627,\n",
              " 'lloberes': 628,\n",
              " 'lloberas': 629,\n",
              " 'tort': 630,\n",
              " 'fontanills': 631,\n",
              " 'galceran': 632,\n",
              " 'bru': 633,\n",
              " 'medicina': 634,\n",
              " 'hierony': 635,\n",
              " 'vernet': 636,\n",
              " 'llorenço': 637,\n",
              " 'nots': 638,\n",
              " 'agustina': 639,\n",
              " 'arno': 640,\n",
              " 'llotge': 641,\n",
              " 'sistaller': 642,\n",
              " 'cardus': 643,\n",
              " 'terme': 644,\n",
              " 'tavarner': 645,\n",
              " 'fost': 646,\n",
              " 'calopa': 647,\n",
              " 'gervasi': 648,\n",
              " 'amor': 649,\n",
              " 'deu': 650,\n",
              " 'cot': 651,\n",
              " 'piquer': 652,\n",
              " 'paleja': 653,\n",
              " 'bodo': 654,\n",
              " 'blanquinador': 655,\n",
              " 'defuct': 656,\n",
              " 'damiana': 657,\n",
              " 'manaut': 658,\n",
              " 'oms': 659,\n",
              " 'martina': 660,\n",
              " 'te': 661,\n",
              " 'xidor': 662,\n",
              " 'callas': 663,\n",
              " 'terinya': 664,\n",
              " 'balmasseda': 665,\n",
              " 'ciutat': 666,\n",
              " 'pro': 667,\n",
              " 'vincia': 668,\n",
              " 'biscaya': 669,\n",
              " 'julia': 670,\n",
              " 'massot': 671,\n",
              " 'forners': 672,\n",
              " 'berga': 673,\n",
              " 'subira': 674,\n",
              " 'sparter': 675,\n",
              " 'hypolita': 676,\n",
              " 'benavent': 677,\n",
              " 'carreter': 678,\n",
              " 'ca': 679,\n",
              " 'therina': 680,\n",
              " 'copons': 681,\n",
              " '3': 682,\n",
              " 'isach': 683,\n",
              " 'totel': 684,\n",
              " 'stamper': 685,\n",
              " 'valentia': 686,\n",
              " 'miranda': 687,\n",
              " 'lucia': 688,\n",
              " 'peralta': 689,\n",
              " 'sacastiella': 690,\n",
              " 'bartres': 691,\n",
              " 'bosca': 692,\n",
              " 'valls': 693,\n",
              " 'solitar': 694,\n",
              " 'salmell': 695,\n",
              " 'terrasola': 696,\n",
              " 'guell': 697,\n",
              " 'monmell': 698,\n",
              " 'angel': 699,\n",
              " 'uguet': 700,\n",
              " 'marmellar': 701,\n",
              " 'vinyes': 702,\n",
              " 'ur': 703,\n",
              " 'sol': 704,\n",
              " 'giro': 705,\n",
              " 'marles': 706,\n",
              " 'ribot': 707,\n",
              " 'genis': 708,\n",
              " 'horta': 709,\n",
              " 'artigues': 710,\n",
              " 'mallorca': 711,\n",
              " 'gaspar': 712,\n",
              " 'serrador': 713,\n",
              " 'sona': 714,\n",
              " 'bernada': 715,\n",
              " 'caramany': 716,\n",
              " 'nar': 717,\n",
              " 'cis': 718,\n",
              " 'cabessa': 719,\n",
              " 'spara': 720,\n",
              " 'guera': 721,\n",
              " 'garau': 722,\n",
              " 'thomasa': 723,\n",
              " 'jover': 724,\n",
              " 'tremp': 725,\n",
              " 'bonet': 726,\n",
              " 'caputxer': 727,\n",
              " 'coloma': 728,\n",
              " 'alegret': 729,\n",
              " 'ginestar': 730,\n",
              " 'serraller': 731,\n",
              " 'roses': 732,\n",
              " 'climent': 733,\n",
              " 'plegamans': 734,\n",
              " 'polonia': 735,\n",
              " 'ebrera': 736,\n",
              " 'castanyer': 737,\n",
              " 'baster': 738,\n",
              " 'martorell': 739,\n",
              " 'gali': 740,\n",
              " 'jaue': 741,\n",
              " 'vendrell': 742,\n",
              " 'perez': 743,\n",
              " 'florentina': 744,\n",
              " 'ayala': 745,\n",
              " 'correu': 746,\n",
              " 'castellet': 747,\n",
              " 'nars': 748,\n",
              " 'lluis': 749,\n",
              " 'fraça': 750,\n",
              " 'campana': 751,\n",
              " 'ayter': 752,\n",
              " 'ramoneda': 753,\n",
              " 'carreras': 754,\n",
              " 'gava': 755,\n",
              " 'barru': 756,\n",
              " 'fet': 757,\n",
              " 'munt': 758,\n",
              " 'torrella': 759,\n",
              " 'estasia': 760,\n",
              " 'igualada': 761,\n",
              " 'rovira': 762,\n",
              " 'carreres': 763,\n",
              " 'balle': 764,\n",
              " 'pujes': 765,\n",
              " 'marquesa': 766,\n",
              " 'molner': 767,\n",
              " 'porter': 768,\n",
              " 'bordes': 769,\n",
              " 'calsas': 770,\n",
              " 'ros': 771,\n",
              " 'prohensals': 772,\n",
              " 'martres': 773,\n",
              " 'isern': 774,\n",
              " 'al': 775,\n",
              " 'pr': 776,\n",
              " 'raurell': 777,\n",
              " 'tegas': 778,\n",
              " 'ella': 779,\n",
              " 'genelose': 780,\n",
              " 'italia': 781,\n",
              " 'pintor': 782,\n",
              " 'febrer': 783,\n",
              " 'saba': 784,\n",
              " 'dell': 785,\n",
              " 'orelles': 786,\n",
              " 'cordas': 787,\n",
              " 'viola': 788,\n",
              " 'guerau': 789,\n",
              " 'bestias': 790,\n",
              " 'salamo': 791,\n",
              " 'claria': 792,\n",
              " 'clariana': 793,\n",
              " 'horts': 794,\n",
              " 'falgueres': 795,\n",
              " 'brions': 796,\n",
              " 'ursol': 797,\n",
              " 'ronsana': 798,\n",
              " 'palaudaries': 799,\n",
              " 'dalta': 800,\n",
              " '9': 801,\n",
              " 'moral': 802,\n",
              " 'bo': 803,\n",
              " 'blay': 804,\n",
              " 'pampinya': 805,\n",
              " 'alavall': 806,\n",
              " 'barba': 807,\n",
              " 'forti': 808,\n",
              " 'grimal': 809,\n",
              " 'co': 810,\n",
              " 'lomer': 811,\n",
              " 'andreua': 812,\n",
              " 'staper': 813,\n",
              " 'pellisser': 814,\n",
              " 'nacijs': 815,\n",
              " 'pallas': 816,\n",
              " 'leyda': 817,\n",
              " 'montalt': 818,\n",
              " 'bas': 819,\n",
              " 'taix': 820,\n",
              " 'corda': 821,\n",
              " 'morell': 822,\n",
              " 'droguer': 823,\n",
              " 'calella': 824,\n",
              " 'folch': 825,\n",
              " '20': 826,\n",
              " 'bravo': 827,\n",
              " 'tapiner': 828,\n",
              " 'diego': 829,\n",
              " 'domingo': 830,\n",
              " 'fornes': 831,\n",
              " 'assaonador': 832,\n",
              " 'habit': 833,\n",
              " 'suert': 834,\n",
              " 'hieronya': 835,\n",
              " 'verder': 836,\n",
              " 'elena': 837,\n",
              " 'gallissa': 838,\n",
              " 'bauras': 839,\n",
              " 'guasch': 840,\n",
              " 'territori': 841,\n",
              " 'fonollar': 842,\n",
              " 'gramanet': 843,\n",
              " 'steva': 844,\n",
              " 'fossar': 845,\n",
              " 'fonolla': 846,\n",
              " 'juqueres': 847,\n",
              " 'serda': 848,\n",
              " 'sendra': 849,\n",
              " 'magrinya': 850,\n",
              " 'requesens': 851,\n",
              " 'ne': 852,\n",
              " 'gociant': 853,\n",
              " 'valetia': 854,\n",
              " 'solanes': 855,\n",
              " 'sarmet': 856,\n",
              " 'maymo': 857,\n",
              " 'monistrol': 858,\n",
              " '2': 859,\n",
              " 'boveres': 860,\n",
              " 'scudaller': 861,\n",
              " 'cecilia': 862,\n",
              " 'llinas': 863,\n",
              " 'catala': 864,\n",
              " 'mossach': 865,\n",
              " 'ugo': 866,\n",
              " 'molin': 867,\n",
              " 'rey': 868,\n",
              " 'carder': 869,\n",
              " 'muntaner': 870,\n",
              " 'cardona': 871,\n",
              " 'micas': 872,\n",
              " 'molinderey': 873,\n",
              " 'florencia': 874,\n",
              " 'grau': 875,\n",
              " 'llicetia': 876,\n",
              " 'moya': 877,\n",
              " 'sovils': 878,\n",
              " 'pla': 879,\n",
              " 'sentis': 880,\n",
              " 'destarach': 881,\n",
              " 'escola': 882,\n",
              " 'pineda': 883,\n",
              " 'eufrasina': 884,\n",
              " 'jorda': 885,\n",
              " 'valentina': 886,\n",
              " 'balsareny': 887,\n",
              " 'castello': 888,\n",
              " 'balsa': 889,\n",
              " 'grano': 890,\n",
              " 'llers': 891,\n",
              " 'christophol': 892,\n",
              " 'organs': 893,\n",
              " 'devall': 894,\n",
              " 'besalu': 895,\n",
              " 'bassa': 896,\n",
              " 'roca': 897,\n",
              " 'menat': 898,\n",
              " 'oriach': 899,\n",
              " 'termens': 900,\n",
              " 'gallart': 901,\n",
              " 'molanta': 902,\n",
              " 'dexen': 903,\n",
              " 'moles': 904,\n",
              " 'bellsola': 905,\n",
              " 'mogoda': 906,\n",
              " 'armengol': 907,\n",
              " 'basso': 908,\n",
              " 'splugues': 909,\n",
              " 'esteve': 910,\n",
              " 'vilarodona': 911,\n",
              " 'senauja': 912,\n",
              " 'tisorer': 913,\n",
              " 'climet': 914,\n",
              " 'dardet': 915,\n",
              " 'guilleumas': 916,\n",
              " 'serdanyola': 917,\n",
              " 'mort': 918,\n",
              " 'hospital': 919,\n",
              " 'mauri': 920,\n",
              " 'geltru': 921,\n",
              " 'galcera': 922,\n",
              " 'urgelles': 923,\n",
              " 'pares': 924,\n",
              " 'hiero': 925,\n",
              " 'nyma': 926,\n",
              " 'ortells': 927,\n",
              " 'brunet': 928,\n",
              " 'palou': 929,\n",
              " 'reptor': 930,\n",
              " 'casansals': 931,\n",
              " 'monpeo': 932,\n",
              " 'pastor': 933,\n",
              " 'homellons': 934,\n",
              " 'po': 935,\n",
              " 'blet': 936,\n",
              " 'dardenya': 937,\n",
              " 'don': 938,\n",
              " 'bellmunt': 939,\n",
              " 'brasser': 940,\n",
              " 'masdovelles': 941,\n",
              " 'massaguera': 942,\n",
              " 'mathia': 943,\n",
              " 'massaguer': 944,\n",
              " 'nego': 945,\n",
              " 'ciant': 946,\n",
              " 'quadriu': 947,\n",
              " 'boxera': 948,\n",
              " 'llibrater': 949,\n",
              " 'basili': 950,\n",
              " 'alexandre': 951,\n",
              " 'pomes': 952,\n",
              " 'tona': 953,\n",
              " 'residencia': 954,\n",
              " 'tries': 955,\n",
              " 'cervello': 956,\n",
              " 'marxant': 957,\n",
              " 'palafolls': 958,\n",
              " 'marca': 959,\n",
              " 'tosinch': 960,\n",
              " 'scalfape': 961,\n",
              " 'bartho': 962,\n",
              " 'meu': 963,\n",
              " 'musich': 964,\n",
              " 'porta': 965,\n",
              " 'luysa': 966,\n",
              " 'rigarda': 967,\n",
              " 'ribafort': 968,\n",
              " 'alsina': 969,\n",
              " 'gorgui': 970,\n",
              " 'corro': 971,\n",
              " 'lloret': 972,\n",
              " 'pujol': 973,\n",
              " 'dionys': 974,\n",
              " 'homs': 975,\n",
              " 'orrius': 976,\n",
              " 'pou': 977,\n",
              " 'molines': 978,\n",
              " 'cervera': 979,\n",
              " 'rostoll': 980,\n",
              " 'extraordinari': 981,\n",
              " 'masramon': 982,\n",
              " 'tra': 983,\n",
              " 'giner': 984,\n",
              " 'miramdeu': 985,\n",
              " 'guerri': 986,\n",
              " 'martha': 987,\n",
              " 'llobet': 988,\n",
              " 'rocafort': 989,\n",
              " 'llo': 990,\n",
              " 'bet': 991,\n",
              " 'sunyol': 992,\n",
              " 'rigual': 993,\n",
              " 'tauder': 994,\n",
              " 'torello': 995,\n",
              " 'vallromanes': 996,\n",
              " 'vilardell': 997,\n",
              " 'defucta': 998,\n",
              " 'arquer': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_oov_words(LUT = LUT, test_set = test_loader):\n",
        "    # Out of vocabulary list\n",
        "    oov_words = []\n",
        "\n",
        "    for i in range(len(test_set)):\n",
        "        for word, label in zip(*test_set[i]):\n",
        "            token = word.lower()  # Lowercase the token\n",
        "\n",
        "            if token not in LUT:\n",
        "                oov_words.append(token) # Add the tokens of the test dataloader to the list if they don't belong to the LUT\n",
        "\n",
        "    return oov_words\n",
        "            \n",
        "print(list(check_oov_words()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxkJj8tDe5WA",
        "outputId": "0d1de246-0ebd-4c1f-85f9-05710d81c14f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['argemir', 'nyella', 'rianna', 'bonastra', 'angli', 'victo', 'theodora', 'payas', 'monllor', 'rotxe', 'moreno', 'islla', 'brasil', 'gusman', 'more', 'islla', 'sto', 'thome', 'habitants', 'melcior', 'bachs', 'pachs', 'plans', 'begas', 'rius', 'galeras', 'box', 'deseny', 'francesa', 'ortiz', 'faneca', 'faneca', 'sobrevila', 'sobrevila', 'cabrer', 'carantela', 'mso', 'tibau', 'monblanch', 'tibau', 'broquets', 'noguera', 'gassull', 'llondra', 'quart', 'pallissa', 'vivints', 'cabus', 'felis', 'buyra', 'scrivent', 'castigaleu', 'comptat', 'ribagossa', 'fontanilles', 'conteso', 'tatare', 'idrach', 'peramon', 'peramon', 'terre', 'arisart', 'arisart', 'faja', 'pinya', 'faliu', 'campanya', 'faliu', 'majol', 'majol', 'guardi#', 'debarca', 'constansa', 'llorenci', 'caxaler', 'llorenci', 'aguller', 'poses', 'miro', 'darder', 'darder', 'garces', 'imaginayre', 'juan#', 'villaro', 'vilademaser', 'muntells', 'muntells', 'sengermes', 'cebriana', 'tamuyell', 'cabaner', 'manader', 'manader', 'campprecios', 'busquet', 'crich', 'spa', 'raguera', 'valta', 'arevig', 'dimarts', 'berenguer', 'alaverni', 'conflent', 'glandina', 'fabrega', 'fabrega', 'fargayre', 'gatuellas', 'darder', 'comi', 'rosa', 'blanquart', 'dimas', 'tisser', 'mandri', 'mandri', 'corties', 'clavetayre', 'cani', 'jonqueres', 'munmany', 'tarafa', 'castellterçol', 'novell', 'novell', 'planta', 'llobre', 'gat', 'testa', 'testa', 'auger', 'ratx', 'ayguardenter', 'perris', 'celles', 'assensio', 'ge', 'febres', 'fabres', 'hor', 'tola', 'sitjar', 'macip', 'ritoreta', 'campderos', 'morros', 'masseres', 'masseres', 'honorat', 'luciana']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to batch computation of some modules, sequences must have constant length. As a common practice, we will create three new tokens ```<bos>``` and ```<eos>``` for the start and the end of a given sequence and ```<unk>``` for unkown tokens in the application (test) layer or 0 padding during the training. Manually add those tokens to the ```LUT```. \n",
        " \n",
        "\n",
        "Under those constraints, fill the corresponding functions that will post-process each batch. Feel free to code more post-processing functions if you need it.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7zHKqdlAUtly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LUT['<unk>'] = len(LUT) + 1\n",
        "LUT['<bos>'] = len(LUT) + 1\n",
        "LUT['<eos>'] = len(LUT) + 1"
      ],
      "metadata": {
        "id": "tnpVY5K7iJ9d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\n",
        "def complete_seq(X) -> List[List]:\n",
        "    '''\n",
        "\n",
        "        Input: \n",
        "        X: A batch of N sequences [\n",
        "            [word1, ..., wordn],\n",
        "            [word1, ..., wordm]\n",
        "        ]\n",
        "\n",
        "        Output:\n",
        "        A batch of N sequences with MAX_SEQUENCE_LENGTH tokens.\n",
        "            - The starting token will always be <sos>\n",
        "            - The last 'real' token <eos>\n",
        "            - Tokens from <eos> until MAX_SEQUENCE_LENGTH will be <unk> as 0 padding.\n",
        "\n",
        "    '''\n",
        "    complete_seq = []\n",
        "    for seq in X:\n",
        "        seq = ['<bos>'] + seq + ['<eos>']\n",
        "        seq += ['<unk>'] * (MAX_SEQUENCE_LENGTH - len(seq))\n",
        "        complete_seq.append(seq[:MAX_SEQUENCE_LENGTH])\n",
        "    return complete_seq\n",
        "\n",
        "def post_process(X, functions = [complete_seq,]):\n",
        "  for f in functions: X = f(X)\n",
        "  return X"
      ],
      "metadata": {
        "id": "oj0cq0SGUsgF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER - Baseline Approach\n",
        "\n",
        "The first approach we will try is based on computing the probabilities for each word in our training corpus. This means computing the most likely category for each word in the dictionary.\n",
        "\n",
        "Compute the test categories predictions and measure the performance for this simple model."
      ],
      "metadata": {
        "id": "GKuJPkJOanmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_emissions_dict(dataloader) -> Dict:\n",
        "    '''\n",
        "\n",
        "        Given the train loader ```dataloader```\n",
        "        this function will compute the max likelihood dictionary for each word.\n",
        "\n",
        "    Input:\n",
        "        dataloader: train loader with EsposallesTextDataset\n",
        "    \n",
        "    Outputs:\n",
        "        Dict: {\n",
        "        pagès: {name: X occupation: X}, # REMEMBER TO LOWER YOUR TOKENS!\n",
        "                ...\n",
        "        LUT - wordn: {category: x%, ...}\n",
        "        }\n",
        "\n",
        "    '''\n",
        "    emissions_dict = {}\n",
        "    \n",
        "    token_counts = Counter()\n",
        "    label_counts = Counter()\n",
        "    token_label_counts = Counter()\n",
        "    \n",
        "\n",
        "    for i in range(len(dataloader)):\n",
        "        for word,label in zip(*dataloader[i]):\n",
        "            token = word.lower()\n",
        "            token_counts[token] += 1\n",
        "            label_counts[label] += 1\n",
        "            token_label_counts[(token,label)] += 1\n",
        "    \n",
        "    for token, label_count in token_label_counts.items():\n",
        "        token, label = token\n",
        "        if token not in emissions_dict:\n",
        "            emissions_dict[token] = {}\n",
        "        emissions_dict[token][label] = label_count / token_counts[token]\n",
        "\n",
        "    return emissions_dict\n"
      ],
      "metadata": {
        "id": "rYA9D-twbego"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priors = compute_emissions_dict(train_loader)"
      ],
      "metadata": {
        "id": "uFQe-5VykF9Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, as an example, your emissions dictionary should yield the following emission:\n",
        "\n",
        "$P(location |$ ```Prats``` $) = 18\\%$\n",
        "\n",
        "$P(surname |$ ```Prats``` $) = 72\\%$\n",
        "\n",
        "$P(other |$ ```Prats``` $) = 9\\%$"
      ],
      "metadata": {
        "id": "OozkEhtlQow-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "priors['prats']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEFUzlRpQmTa",
        "outputId": "d10dc1ff-afbd-4f19-8458-937a3fb90a35"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'location': 0.18181818181818182,\n",
              " 'surname': 0.7272727272727273,\n",
              " 'other': 0.09090909090909091}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method has its limitations in terms of lack of context and, therefore, low expresivity. \n",
        "\n",
        "The following function will compute the confusion matrix for the predictions in the ```test_set``` in order to find the most problematic words. \n",
        "\n",
        "* What do they all have in common? \n",
        "* What kind of words are the least performers?\n",
        "* What's your solution for out-of-vocabulary words? Can you provide a prediction for those?"
      ],
      "metadata": {
        "id": "p1JaHhJvcwK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What do they all have in common?\n",
        "\n",
        "The problematic words in the baseline model often have commonalities such as sharing similar features or having ambiguous meanings. These words might have multiple possible tags, making it difficult for the model to correctly predict their tags without considering context.\n",
        "\n",
        "2. What kind of words are the least performers?\n",
        "\n",
        "The least performing words are generally those that have multiple possible tags or meanings depending on the context in which they are used. For example, words like \"bank\" could be an organization (B-ORG) or a location (B-LOC) depending on the context. These words are challenging for the baseline model since it does not take context into account when making predictions.\n",
        "\n",
        "3. What's your solution for out-of-vocabulary words?\n",
        "\n",
        "For this problem we decide to use the most frequent label for OOV words."
      ],
      "metadata": {
        "id": "lvhms-owNTUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test_set(emissions, test_set):\n",
        "    \n",
        "    '''\n",
        "\n",
        "    s: casament eduard pages\n",
        "\n",
        "    prediccio: [None, nom, ofici]\n",
        "\n",
        "    Important: \n",
        "        Remember to check if you can provide a label for each word (OOVs?).\n",
        "        What's your solution for those you cannot classify? Justify.\n",
        "\n",
        "        tip: be as creative as you want.\n",
        "\n",
        "    '''\n",
        "    predictions = []\n",
        "    for i in range(len(test_set)):\n",
        "            seq_predictions = []\n",
        "            for word, label in zip(*test_set[i]):\n",
        "                token = word.lower()\n",
        "                if token in emissions:\n",
        "                    pred_label = max(emissions[token], key=emissions[token].get)\n",
        "                else:\n",
        "                    # Use the most frequent label for OOV words (simple but effective)\n",
        "                    pred_label = 'other'\n",
        "                seq_predictions.append(pred_label)\n",
        "\n",
        "            predictions.append(seq_predictions)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_test_set(priors, test_loader)\n",
        "\n",
        "def find_common_errors(x_test: List[List], y_pred: List[List], y_true: List[List]) -> Dict:\n",
        "    '''\n",
        "        Input: \n",
        "        x_test: A list with each sample in the corpus with the words for which we\n",
        "    ran each prediction\n",
        "            [\n",
        "            ['lorem', 'ipsum', 'dolor', 'sit', 'amet'],\n",
        "            ['Hello', 'world', '!!!'],\n",
        "            ]\n",
        "        \n",
        "        y_pred: A list with the predicted labels for each word in x_test corpus.\n",
        "            [\n",
        "            ['1', '0', '0', '1', '2'],\n",
        "            ['2', '1', '0'],\n",
        "            ]\n",
        "        \n",
        "        y_true: GT for the x_test sample\n",
        "            [\n",
        "            ['0', '0', '0', '1', '2'],\n",
        "            ['0', '1', '0'],\n",
        "            ]\n",
        "        {\n",
        "        pages: [{'pred': prediction, gt: label}, {'pred': prediction, 'gt': label}, ...]\n",
        "        }\n",
        "    '''\n",
        "    x_test_words = [word for List in x_test for word in List] #this would be equivalent to:     x_test_words = []\n",
        "                                                              #                                 for List in x_test:\n",
        "                                                              #                                     for word in list:\n",
        "                                                              #                                         x_test_words.append(word)\n",
        "    y_pred_labels = [label for List in y_pred for label in List]\n",
        "    y_true_labels = [label for List in y_true for label in List]\n",
        "\n",
        "    errors = {word: {\"pred\": pred, 'gt': gt} for word, pred, gt in zip(x_test_words, y_pred_labels, y_true_labels) if pred != gt}\n",
        "\n",
        "    return errors\n",
        "\n",
        "def compute_token_precision(x_test: List[List], y_pred: List[List], y_true: List[List]):\n",
        "    \n",
        "    errors = find_common_errors(x_test, y_pred, y_true)\n",
        "   \n",
        "    x_test_words = [word for List in x_test for word in List]\n",
        "    error_rate = len(errors)/len(x_test_words)\n",
        "    \n",
        "    return 1 - error_rate"
      ],
      "metadata": {
        "id": "lDQeZ1_Zcbqx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_common_errors([test_loader[idx][0] for idx in range(len(test_loader))], predictions, [test_loader[idx][1] for idx in range(len(test_loader))])['Esteva']\n",
        "compute_token_precision([test_loader[idx][0] for idx in range(len(test_loader))], predictions, [test_loader[idx][1] for idx in range(len(test_loader))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-bjHuqWLH_w",
        "outputId": "10214256-9863-49c1-dd88-0d75ff4f52e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9308014161570647"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "Here, write a brief conclusion for this notebook reffering to the main differences, advantages and disadvantages for each approach.\n"
      ],
      "metadata": {
        "id": "F3OE0v8UMy_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Your conclusions here\n",
        "\n",
        "After analyzing the given code, we can draw the following conclusions:\n",
        "\n",
        "The task at hand is Named Entity Recognition (NER) and sequence labeling in historical handwritten documents. The goal is to extract information from these documents and categorize the entities into semantic categories such as family names, places, occupations, etc.\n",
        "\n",
        "The code uses a simple baseline approach to predict the named entity categories, which involves computing the most likely category for each word based on the training data.\n",
        "\n",
        "The baseline approach does not take context into consideration and is limited in its expressiveness. It also faces challenges in handling out-of-vocabulary (OOV) words. The current solution for OOV words is to assign them the most frequent label, which may not be the best approach in all cases.\n",
        "\n",
        "The confusion matrix and common errors are computed to analyze the model's performance. The most problematic words are identified and analyzed for any common patterns or characteristics.\n",
        "\n",
        "Based on these conclusions, the baseline approach can be improved upon by incorporating context-aware methods or other techniques for handling OOV words. These improvements could potentially lead to better performance and more accurate predictions for the NER task in historical handwritten documents."
      ],
      "metadata": {
        "id": "j_tAgE_6M-Y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HMM Approach\n",
        "\n",
        "As demonstrated in the previous experiment, using just the priors have not enough expresivity for managing both out of vocabulary words and polysemic words. Here we will use the ```python-crfsuite``` module to build a Hidden Markov Model and improve the predictions on ```test_set```.\n",
        "\n",
        "Check <a href = 'https://python-crfsuite.readthedocs.io/en/latest/'>here</a> the  ```python-crfsuite``` documentation.\n",
        "\n",
        "First, we will set up the parameters for our CRF model.\n"
      ],
      "metadata": {
        "id": "NGLLJH44fUce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_to_hmm_features(sent, i):\n",
        "\n",
        "\n",
        "    '''\n",
        "     Reminder: \n",
        "        The Markov assumption states that the transition for the i-th token\n",
        "          depends on the (i-1)-th token. \n",
        "\n",
        "\n",
        "    '''\n",
        "    word, _ = sent[i]\n",
        "    #emission probilities\n",
        "    features = [\n",
        "        'bias',\n",
        "        'word.lower=' + word.lower(),\n",
        "    ]\n",
        "    if i == 0:\n",
        "        features.append('bos')\n",
        "\n",
        "    if (i == len(sent) - 1):\n",
        "        features.append('eos')\n",
        "                \n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2HMMfeatures(sent):\n",
        "    return [get_word_to_hmm_features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, label in sent]    \n",
        "\n"
      ],
      "metadata": {
        "id": "lnnMYLlp94mz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the dataset\n",
        "# to the (token, gt) tuple format\n",
        "train_sents = [  [(x,y) for x,y in zip(*train_loader[idx])] for idx in range(len(train_loader))]\n",
        "test_sents =  [  [(x,y) for x,y in zip(*train_loader[idx])] for idx in range(len(test_loader))]"
      ],
      "metadata": {
        "id": "Jz5HDNLApC5U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [sent2HMMfeatures(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2HMMfeatures(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]"
      ],
      "metadata": {
        "id": "nlPACtd-kFlv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = crfs.Trainer(verbose=False) # Instance a CRF trainer\n",
        "\n",
        "for xseq, yseq in zip(X_train, y_train):\n",
        "    trainer.append(xseq, yseq) # Stack the data"
      ],
      "metadata": {
        "id": "jIOvOkloh8lr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.set_params({\n",
        "    'c1': 1.0,   # coefficient for L1 penalty\n",
        "    'c2': 1e-3,  # coefficient for L2 penalty\n",
        "    'max_iterations': 50,  # Max Number of iterations for the iterative algorithm\n",
        "\n",
        "    # include transitions that are possible, but not observed (smoothing)\n",
        "    'feature.possible_transitions': True\n",
        "})"
      ],
      "metadata": {
        "id": "SGN9gHxQsiX3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.train('npl_ner_crf.crfsuite') # Train the model and save it locally."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nSheENbslnu",
        "outputId": "cbb8d35c-5c64-409a-8993-feba93e3cdcf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 396 ms, sys: 3.21 ms, total: 399 ms\n",
            "Wall time: 403 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = crfs.Tagger()\n",
        "tagger.open('npl_ner_crf.crfsuite') # Load the inference API"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqj33gJNswgI",
        "outputId": "4b6fcddf-0bce-4f47-9061-d6db5bf4f607"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.closing at 0x7f4601ed8040>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_sent = test_sents[0]\n",
        "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')\n",
        "\n",
        "print(\"Predicted:\", ' '.join(tagger.tag(sent2HMMfeatures(example_sent))))\n",
        "print(\"Correct:  \", ' '.join(sent2labels(example_sent))) # Inference"
      ],
      "metadata": {
        "id": "Dn2xvzkR6ysD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f196fe-2446-47a1-a16c-164f25311b36"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dilluns a 5 rebere de Hyacinto Boneu hortola de Bara fill de Juan Boneu parayre defunct y de Maria ab Anna donsella filla de t Cases pages de Bara defunct y de Peyrona\n",
            "\n",
            "Predicted: other other other other other name surname occupation other location other other name surname occupation other other other name other name state other other name surname occupation other location other other other name\n",
            "Correct:   other other other other other name surname occupation other location other other name surname occupation other other other name other name state other other name surname occupation other location other other other name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "def bio_classification_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "\n",
        "    Classification report.\n",
        "    You can use this as evaluation for both in the baseline model and new model.\n",
        "\n",
        "    \"\"\"\n",
        "    lb = LabelBinarizer()\n",
        "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
        "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
        "        \n",
        "    tagset = set(lb.classes_) - {'O'}\n",
        "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
        "    \n",
        "    return classification_report(\n",
        "        y_true_combined,\n",
        "        y_pred_combined,\n",
        "        labels = [class_indices[cls] for cls in tagset],\n",
        "        target_names = tagset,\n",
        "    )"
      ],
      "metadata": {
        "id": "TEJ7BVnQtGYr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the predictions \n",
        "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
        "y_pred"
      ],
      "metadata": {
        "id": "eS74uUQ-tNae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f026f5b7-bb0a-4500-8fc6-5adeb1936a01"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'state',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'other',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'other',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'state'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'surname'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'name',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name'],\n",
              " ['other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'location',\n",
              "  'location',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'state',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name',\n",
              "  'surname',\n",
              "  'occupation',\n",
              "  'other',\n",
              "  'other',\n",
              "  'other',\n",
              "  'name']]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use the  ```bio_classification_report``` function in both the Baseline model and the HMM model. Do you observe any improvement? In which cases does it still fail?\n",
        "\n"
      ],
      "metadata": {
        "id": "pernKQrdd0uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_predictions = predict_test_set(priors, test_loader)\n",
        "baseline_true = [labels for _, labels in [test_loader[idx] for idx in range(len(test_loader))]]\n",
        "\n",
        "hmm_predictions = y_pred\n"
      ],
      "metadata": {
        "id": "roiG9SO6svCU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Baseline model classification report:\")\n",
        "print(bio_classification_report(baseline_true, baseline_predictions))\n",
        "\n",
        "print(\"\\nHMM model classification report:\")\n",
        "print(bio_classification_report(y_test, hmm_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKEpw_SFs1D_",
        "outputId": "87abe252-b998-41c8-b0f5-896895ad127e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline model classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    location       0.94      0.71      0.81       462\n",
            "        name       0.94      0.95      0.94       494\n",
            "  occupation       0.94      0.85      0.90       294\n",
            "       other       0.85      0.99      0.91      1493\n",
            "       state       0.98      0.95      0.96       113\n",
            "     surname       0.84      0.47      0.60       251\n",
            "\n",
            "   micro avg       0.89      0.89      0.89      3107\n",
            "   macro avg       0.91      0.82      0.85      3107\n",
            "weighted avg       0.89      0.89      0.88      3107\n",
            " samples avg       0.89      0.89      0.89      3107\n",
            "\n",
            "\n",
            "HMM model classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    location       0.95      0.95      0.95       416\n",
            "        name       0.98      0.98      0.98       487\n",
            "  occupation       0.93      0.97      0.95       296\n",
            "       other       0.99      0.98      0.98      1486\n",
            "       state       0.97      0.98      0.97       114\n",
            "     surname       0.97      0.94      0.95       269\n",
            "\n",
            "   micro avg       0.97      0.97      0.97      3068\n",
            "   macro avg       0.96      0.97      0.97      3068\n",
            "weighted avg       0.97      0.97      0.97      3068\n",
            " samples avg       0.97      0.97      0.97      3068\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can notice many improvements in the HMM model because the baseline model is a simple approach that primarily relies on emission probabilities, which are the probabilities of observing a particular word given a label. While this approach may work well for some cases, it still has several limitations:\n",
        "\n",
        "1. **Lack of context:** The baseline model does not consider the context in which a word appears. It only considers the individual word and its relationship with the label. This can lead to incorrect predictions when the meaning of a word or its role in a sentence depends on the surrounding words or the overall context.\n",
        "\n",
        "2. **Out-of-vocabulary words:** The baseline model may perform poorly on out-of-vocabulary (OOV) words, i.e., words that are not present in the training set. For OOV words, the model assigns the most frequent label for all such words, which is a simplistic approach and may not always be accurate.\n",
        "\n",
        "3. **Label transitions:** The baseline model does not take into account the transitions between labels, which may provide valuable information for Named Entity Recognition (NER) tasks. For instance, certain labels may be more likely to follow or precede specific labels, but the baseline model does not leverage this information.\n",
        "\n",
        "4. **No higher-order dependencies:** The baseline model doesn't consider higher-order dependencies between words and labels in a sentence. For example, it can't capture relationships between non-adjacent words, which might be important for determining the correct label."
      ],
      "metadata": {
        "id": "6hY8PupDuses"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantitative and Qualitative analysis\n"
      ],
      "metadata": {
        "id": "_ngRwxGX_EtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally, the HMM model is expected to outperform the Baseline model as it takes into account the dependencies between the tags in the sequence. However, there might still be cases where the HMM model fails. These can include:\n",
        "\n",
        "1. Ambiguity in the data: If there is ambiguity in the data itself or if there are inconsistencies in the way the data has been annotated, both the Baseline and HMM models may struggle to perform well in these cases.\n",
        "\n",
        "2. Rare or unseen words/entities: The HMM model may fail in cases where it encounters rare or previously unseen words/entities during testing. In such cases, the model may not have enough information to accurately predict the correct tags.\n",
        "\n",
        "3. Complex dependencies between tags: If there are complex dependencies between tags in the sequence that the HMM model is unable to capture, it may still fail to make accurate predictions.\n",
        "\n",
        "To better understand the specific cases where the HMM model fails, you can analyze the confusion matrix and identify the most common misclassifications. This can give you insights into potential areas of improvement for your model or the need for additional training data."
      ],
      "metadata": {
        "id": "mJma1-vSrzJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Can this model provide a solution for out-of-vocabulary words?\n",
        "* Can you provide examples of words which changed its category compared to the max-likelihood prior when introducing context? See the following example.\n",
        "\n",
        "$P($ ```Noun |people``` $) = 80\\%$ \n",
        "\n",
        "$P($ ``` people, Noun | \"a  planet\" ``` $) = 5\\%$\n",
        "\n",
        "$P($ ``` people, Verb | \"a  planet\" ``` $) = 90\\%$\n",
        "\n",
        "* How does it perform with respect to the out-of-vocabulary words? e.g. what's the precision for those?\n",
        "\n",
        "*  The following function shows the less likely and most likely transitions. Comment them and perform a deep analysis on each transition, do they have something in common with the errors you found?"
      ],
      "metadata": {
        "id": "QYr3LJuueuIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hidden Markov Model (HMM) can partially address the issue of out-of-vocabulary (OOV) words, but not entirely. One way to handle OOV words in HMMs is to assign them an average or uniform emission probability across all labels. While this approach is better than the baseline model, which assigns the most frequent label, it's still not perfect, as it doesn't capture the true relationship between the OOV word and the labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "smFZEnt1lWd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding the second part of your question, here's an example illustrating how context can influence label predictions when compared to the max-likelihood prior:\n",
        "\n",
        "Consider the sentence: \"John visited San Francisco and loved the Golden Gate Bridge.\"\n",
        "\n",
        "Without context, the max-likelihood prior might label \"Golden\" and \"Gate\" as follows:\n",
        "\n",
        "\"**Golden**\" - B-MISC (miscellaneous entities)\n",
        "\"**Gate**\" - O (no entity)\n",
        "These labels are based on the highest emission probability for the individual words, ignoring the context.\n",
        "\n",
        "However, when we introduce context with an HMM, we can capture the relationship between adjacent words and labels, and we might get the following labels:\n",
        "\n",
        "\"**Golden**\" - B-LOC (location)\n",
        "\"**Gate**\" - I-LOC (location)\n",
        "In this case, the HMM is able to recognize that \"Golden Gate\" together forms a location entity, \"Golden Gate Bridge.\""
      ],
      "metadata": {
        "id": "XMKUVQYdvsDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "info = tagger.info()\n",
        "\n",
        "def print_transitions(trans_features):\n",
        "    for (label_from, label_to), weight in trans_features:\n",
        "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
        "\n",
        "print(\"Top likely transitions:\")\n",
        "print_transitions(Counter(info.transitions).most_common(15))\n",
        "\n",
        "print(\"\\nTop unlikely transitions:\")\n",
        "print_transitions(Counter(info.transitions).most_common()[-15:])"
      ],
      "metadata": {
        "id": "4SCZTNwetk9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e421665-b431-4781-c4eb-dce85b27c727"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top likely transitions:\n",
            "surname -> occupation 5.019786\n",
            "location -> location 4.249178\n",
            "occupation -> occupation 4.194908\n",
            "name   -> surname 3.918272\n",
            "surname -> surname 3.294527\n",
            "other  -> name    2.642988\n",
            "name   -> name    2.280106\n",
            "other  -> location 1.814986\n",
            "occupation -> other   1.799188\n",
            "state  -> state   1.515564\n",
            "name   -> state   1.182575\n",
            "other  -> other   1.181984\n",
            "state  -> other   0.735261\n",
            "occupation -> state   0.467426\n",
            "location -> other   0.403282\n",
            "\n",
            "Top unlikely transitions:\n",
            "state  -> occupation 0.214062\n",
            "surname -> state   0.070542\n",
            "location -> occupation -0.072833\n",
            "name   -> other   -0.506374\n",
            "occupation -> name    -0.617364\n",
            "other  -> surname -0.626089\n",
            "occupation -> location -0.735001\n",
            "surname -> name    -0.998662\n",
            "other  -> occupation -1.030786\n",
            "other  -> state   -1.296538\n",
            "name   -> occupation -1.650451\n",
            "occupation -> surname -2.111763\n",
            "name   -> location -2.166658\n",
            "location -> surname -2.771394\n",
            "location -> name    -3.351583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_state_features(state_features):\n",
        "    for (attr, label), weight in state_features:\n",
        "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
        "\n",
        "print(\"Top positive:\")\n",
        "print_state_features(Counter(info.state_features).most_common(20))\n",
        "\n",
        "print(\"\\nTop negative:\")\n",
        "print_state_features(Counter(info.state_features).most_common()[-20:])"
      ],
      "metadata": {
        "id": "WmxKfyFDtq8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39abe95f-6ec1-45d0-a36f-6fa4ce4ddba4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top positive:\n",
            "10.471530 state  word.lower=viudo\n",
            "9.201264 state  word.lower=donsella\n",
            "9.171407 other  word.lower=ab\n",
            "9.073835 other  word.lower=fill\n",
            "9.043569 other  word.lower=defuncts\n",
            "8.975021 other  word.lower=#\n",
            "8.538504 state  word.lower=viuda\n",
            "8.204700 other  word.lower=defunct\n",
            "7.974212 other  word.lower=y\n",
            "7.971931 other  word.lower=defuncta\n",
            "7.816721 location word.lower=frances\n",
            "7.655355 state  word.lower=dosella\n",
            "7.522998 other  word.lower=rebere\n",
            "7.259508 other  word.lower=habitant\n",
            "7.223392 location word.lower=bara\n",
            "7.056602 other  word.lower=a\n",
            "6.963770 other  word.lower=de\n",
            "6.655697 other  word.lower=filla\n",
            "6.586878 other  word.lower=habitat\n",
            "6.535491 occupation word.lower=llana\n",
            "\n",
            "Top negative:\n",
            "0.009387 occupation word.lower=pastisser\n",
            "0.006814 surname word.lower=pere\n",
            "-0.000147 name   word.lower=sr\n",
            "-0.000667 location word.lower=dels\n",
            "-0.015598 location word.lower=menat\n",
            "-0.061897 surname word.lower=vila\n",
            "-0.086558 surname word.lower=del\n",
            "-0.118397 surname word.lower=toni\n",
            "-0.285311 occupation bias\n",
            "-0.398388 location word.lower=habitant\n",
            "-0.422007 surname eos\n",
            "-0.499756 location word.lower=pages\n",
            "-0.504381 surname word.lower=texidor\n",
            "-0.558585 surname word.lower=y\n",
            "-0.694049 surname word.lower=#\n",
            "-0.757626 state  bias\n",
            "-1.027974 occupation word.lower=del\n",
            "-1.104222 location word.lower=en\n",
            "-1.170696 surname word.lower=serrat\n",
            "-1.568957 location word.lower=domiciliat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we can see that there is a strong association between some of the label pairs:\n",
        "\n",
        "**surname** -> **occupation** and **occupation** -> **occupation**: Indicates that the model recognizes that an occupation often follows a surname and that consecutive words can belong to the occupation category.\n",
        "**location** -> **location**: Shows that the model can identify consecutive location-related words.\n",
        "**name** -> **surname** and **surname** -> **surname**: Indicates that the model understands the relationship between names and surnames, and that consecutive surnames may appear.\n",
        "Top unlikely transitions show the transitions between labels that the model believes are least probable. Some interesting observations here are:\n",
        "\n",
        "**location** -> **name**, **location** -> **surname**, and **location** -> **occupation**: These imply that the model has learned that locations rarely transition directly to names, surnames, or occupations.\n",
        "**name** -> **occupation**, **occupation** -> **surname**, and **occupation** -> **name**: The model has learned that transitioning from a name to an occupation or vice versa is not common, as well as from an occupation to a surname.\n",
        "These unlikely transitions might be related to some errors found in the model. For instance, if the model has learned that transitioning from a location to a surname is unlikely, it may struggle to identify correct sequences where this transition happens."
      ],
      "metadata": {
        "id": "cUUGnATxxICk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Exploration\n",
        "\n",
        "In the definition of the model we used some default hyperparameters related to the training algorithm.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "trainer.set_params({\n",
        "    'c1': 1.0,   # coefficient for L1 penalty\n",
        "    'c2': 1e-3,  # coefficient for L2 penalty\n",
        "    'max_iterations': 50,  # Max Number of iterations for the iterative algorithm\n",
        "\n",
        "    # include transitions that are possible, but not observed (smoothing)\n",
        "    'feature.possible_transitions': True\n",
        "})\n",
        "```\n",
        "Can you improve the precision by better parametrization? Feel free to explore more parameters through [the documentation](https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#sklearn_crfsuite.CRF).\n"
      ],
      "metadata": {
        "id": "kmN-eYcyNP-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New parameters and results here\n",
        "## Use the functions above, no need to re-work.\n",
        "## There is no need of a deep analysis nor qualitative evaluation\n",
        "\n",
        "\n",
        "# You can define your mode with differente parameters such that:\n",
        "param_grid = {\n",
        "    'c1': [0.01, 0.1, 1, 10], # Coefficient for L1 regularization penalty\n",
        "    'c2': [0.01, 0.1, 1, 10], # Coefficient for L2 regularization penalty\n",
        "    \n",
        "    # Different algorithms (Gradient descent using the L-BFGS method\n",
        "\n",
        "    'algorithm': ['lbfgs', # 'l2sgd' - Stochastic Gradient Descent with L2 regularization term\n",
        "                  'ap', # 'ap' - Averaged Perceptron\n",
        "                  'pa', # 'pa' - Passive Aggressive (PA)\n",
        "                  'arow'], # 'arow' - Adaptive Regularization Of Weight Vector (AROW) )\n",
        "\n",
        "    'max_iterations': 100, # Maximum number of iterations\n",
        "    'epsilon':00.1         # The epsilon parameter that determines the condition of convergence.                        \n",
        "}   # And many other parameters"
      ],
      "metadata": {
        "id": "NIPpnIfJf1w6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRF Approach\n",
        "\n",
        "Additionaly, we can address the problem by adding complexity to the transition probabilities. In contrast to HMM, a CRF isn't subject to locality constraints when computing the posterior probabilities.\n",
        "\n",
        "Implement a CRF word featurer that takes into account tokens beyond adjacent ones and your expected needs given the qualitative evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "J4lJF5SrkVHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_to_crf_features(sentence, word_idx):\n",
        "    word, _ = sentence[word_idx]\n",
        "\n",
        "    features = [\n",
        "        'bias',\n",
        "        'word.lower=' + word.lower(),\n",
        "    ]\n",
        "\n",
        "    if word_idx > 0:\n",
        "        prev_word, _ = sentence[word_idx - 1]\n",
        "        features.extend([\n",
        "            '-1:word.lower=' + prev_word.lower(),\n",
        "        ])\n",
        "    else:\n",
        "        features.append('bos')\n",
        "\n",
        "    if word_idx < len(sentence) - 1:\n",
        "        next_word, _ = sentence[word_idx + 1]\n",
        "        features.extend([\n",
        "            '+1:word.lower=' + next_word.lower(),\n",
        "        ])\n",
        "    else:\n",
        "        features.append('eos')\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def get_sent_to_crf_features(sentence):\n",
        "    return [get_word_to_crf_features(sentence, i) for i in range(len(sentence))]\n"
      ],
      "metadata": {
        "id": "hdB-tM1qNPtP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [get_sent_to_crf_features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [get_sent_to_crf_features(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]\n",
        "\n",
        "trainer_crf = crfs.Trainer(verbose=False) # Instance a CRF trainer\n",
        "\n",
        "for xseq, yseq in zip(X_train, y_train):\n",
        "    trainer_crf.append(xseq, yseq) # Stack the data"
      ],
      "metadata": {
        "id": "oOf5yEumg0uE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_crf.set_params({\n",
        "    'c1': 0.15,  # coefficient for L1 penalty\n",
        "    'c2': 0.15,  # coefficient for L2 penalty\n",
        "})"
      ],
      "metadata": {
        "id": "mscLAiChg3wa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_crf.train('npl_ner_crf-improved.crfsuite') # Train the model and save it locally.\n",
        "tagger_crf = crfs.Tagger()\n",
        "tagger_crf.open('npl_ner_crf.crfsuite') # Load the inference API"
      ],
      "metadata": {
        "id": "WYnITa2ahaug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9edcea4-0b6e-45ab-b225-a770a8e2027f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.closing at 0x7f4601ebeb50>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Did the results improve? Comment your decision on the features used in the CRF approach. Is there a difference between using just adjacent tokens or unconstrained optimization? "
      ],
      "metadata": {
        "id": "uhFqQnvOhzA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using just adjacent tokens in the CRF model might limit its ability to capture the context in which a word appears. By using an unconstrained optimization, the CRF model can consider a wider range of tokens and better understand the relationships between words and labels, which could lead to improved results. However, it is important to evaluate the performance of the model on the test set to see if this approach actually leads to better predictions."
      ],
      "metadata": {
        "id": "zT1lnNgasE7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Quantitative and qualitative evaluation\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_crf = [tagger_crf.tag(x) for x in X_test]\n",
        "\n",
        "# Evaluate the CRF model\n",
        "report_crf = bio_classification_report(y_test, y_pred_crf)\n",
        "print(\"CRF Model:\")\n",
        "print(report_crf)\n"
      ],
      "metadata": {
        "id": "ypNM0UfqhtLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd28555-f6ab-4f44-d78b-20712ce2002b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRF Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    location       0.95      0.95      0.95       416\n",
            "        name       0.98      0.98      0.98       487\n",
            "  occupation       0.93      0.97      0.95       296\n",
            "       other       0.99      0.98      0.98      1486\n",
            "       state       0.97      0.98      0.97       114\n",
            "     surname       0.97      0.94      0.95       269\n",
            "\n",
            "   micro avg       0.97      0.97      0.97      3068\n",
            "   macro avg       0.96      0.97      0.97      3068\n",
            "weighted avg       0.97      0.97      0.97      3068\n",
            " samples avg       0.97      0.97      0.97      3068\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the HMM model\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_hmm = [tagger.tag(x) for x in X_test]\n",
        "\n",
        "# Evaluate the HMM model\n",
        "report_hmm = bio_classification_report(y_test, y_pred_hmm)\n",
        "print(\"HMM Model:\")\n",
        "print(report_hmm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FHw_Ih56Mev",
        "outputId": "cce9e634-2316-4b51-9ac5-8f9b23196817"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    location       0.95      0.95      0.95       416\n",
            "        name       0.98      0.98      0.98       487\n",
            "  occupation       0.93      0.97      0.95       296\n",
            "       other       0.99      0.98      0.98      1486\n",
            "       state       0.97      0.98      0.97       114\n",
            "     surname       0.97      0.94      0.95       269\n",
            "\n",
            "   micro avg       0.97      0.97      0.97      3068\n",
            "   macro avg       0.96      0.97      0.97      3068\n",
            "weighted avg       0.97      0.97      0.97      3068\n",
            " samples avg       0.97      0.97      0.97      3068\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results for both models were the same for all labels"
      ],
      "metadata": {
        "id": "BTpusV4WB9vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "Here, write a brief conclusion for this notebook reffering to the main differences, advantages and disadvantages for each approach.\n"
      ],
      "metadata": {
        "id": "Qa8EqtjvisI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> In this notebook, we explored three different methods for Named Entity Recognition (NER) using the BIO tagging scheme: a baseline model, a Hidden Markov Model (HMM), and a Conditional Random Field (CRF) model. We analyzed the performance of each model and discussed their strengths and weaknesses.\n",
        "\n",
        ">In conclusion, the choice of the NER model depends on the specific requirements of the task and the resources available. The baseline model can be used for quick and simple tasks, while the HMM and CRF models offer better performance at the cost of increased complexity and computational resources. The CRF model is particularly well-suited for complex NER tasks that require the incorporation of diverse features and non-local context.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ak0r9CPEjEb0"
      }
    }
  ]
}
