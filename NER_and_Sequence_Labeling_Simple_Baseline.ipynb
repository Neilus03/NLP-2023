{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neilus03/NLP-2023/blob/main/NER_and_Sequence_Labeling_Simple_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Sequence Labeling - NER\n",
        "\n",
        "The extraction of relevant information from historical handwritten document collections is one of the key steps in order to make these manuscripts available for access and searches. In this context, instead of a pure transcription, the objective is to move towards document understanding. Concretely, the aim is to detect the named entities and assign each of them a semantic category, such as family names, places, occupations, etc.\n",
        "\n",
        "\n",
        "A typical application scenario of named entity recognition is demographic documents, since they contain people's names, birthplaces, occupations, etc. In this scenario, the extraction of the key contents and its storage in databases allows the access to their contents and envision innovative services based in genealogical, social or demographic searches.\n",
        "\n",
        "<p style = 'text-align: center'>\n",
        "<img src = \"http://dag.cvc.uab.es/wp-content/uploads/2016/07/esposalla_detall.jpg\">\n",
        "</p>\n",
        "\n",
        "For further doubts and questions, refer to oriol.ramos@uab.cat and alicia.fornes@uab.cat.\n",
        "\n",
        "Usage of Google Colab is not mandatory, but highly recommended as most of the behaviors are expected for a Linux VM with IPython bindings.\n",
        "\n"
      ],
      "metadata": {
        "id": "iXYMGLM2UTG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, we will install the unmet dependencies.\n",
        "\n",
        "This will download some packages and the required data, it may take a while."
      ],
      "metadata": {
        "id": "g9MEafl2I4TN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8Rcfr3oUAms"
      },
      "outputs": [],
      "source": [
        "#@title \n",
        "from IPython.display import clear_output\n",
        "\n",
        "!git clone https://github.com/EauDeData/nlp-resources\n",
        "!cp -r nlp-resources/ resources/\n",
        "!rm -rf nlp-resources/\n",
        "\n",
        "\n",
        "!pip install nltk \n",
        "!pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite\n",
        "clear_output()\n",
        "\n",
        "from typing import * \n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "import sklearn_crfsuite as crfs\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "from resources.data.dataloaders import EsposallesTextDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Curation\n",
        "Loading the dataset"
      ],
      "metadata": {
        "id": "2LbPYoxkYGUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "train_loader = EsposallesTextDataset('resources/data/esposalles/') \n",
        "test_loader = copy.deepcopy(train_loader)\n",
        "test_loader.test()"
      ],
      "metadata": {
        "id": "NRLKbU-4UQ8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of data from each loader:\n",
        "> Format string: ```word```:```label```\n"
      ],
      "metadata": {
        "id": "JzOLIKMGUGAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([f\"{x}:{y}\" for x,y in zip(*train_loader[0])])\n",
        "print([f\"{x}:{y}\" for x,y in zip(*test_loader[0])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6nElJ2-ZCJe",
        "outputId": "bda3ef97-2ea2-42c6-989a-17d09588a041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dilluns:other', 'a:other', '5:other', 'rebere:other', 'de:other', 'Hyacinto:name', 'Boneu:surname', 'hortola:occupation', 'de:other', 'Bara:location', 'fill:other', 'de:other', 'Juan:name', 'Boneu:surname', 'parayre:occupation', 'defunct:other', 'y:other', 'de:other', 'Maria:name', 'ab:other', 'Anna:name', 'donsella:state', 'filla:other', 'de:other', 't:name', 'Cases:surname', 'pages:occupation', 'de:other', 'Bara:location', 'defunct:other', 'y:other', 'de:other', 'Peyrona:name']\n",
            "['Divendres:other', 'a:other', '18:other', 'rebere:other', 'de:other', 'Juan:name', 'Torres:surname', 'pages:occupation', 'habitant:other', 'en:other', 'Sabadell:location', 'fill:other', 'de:other', 'Bernat:name', 'Torres:surname', 'pages:occupation', 'de:other', 'Moya:location', 'bisbat:location', 'de:location', 'Vich:location', 'y:other', 'de:other', 'Antiga:name', 'defucts:other', 'ab:other', 'Margarida:name', 'donsella:state', 'filla:other', 'de:other', 'Juan:name', 'Argemir:surname', 'pages:occupation', 'de:other', 'Sabadell:location', 'y:other', 'de:other', 'Aldonsa:name', 'defuncts:other']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is correctly downloaded you will see two different samples above, and both tests passed below."
      ],
      "metadata": {
        "id": "PXOg5M3-lg5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "# Dataset ckeck\n",
        "\n",
        "for idx in range(len(train_loader)):\n",
        "\n",
        "  x, y = train_loader[idx]\n",
        "  if len(x) != len(y): \n",
        "    print('train_set test not passed')\n",
        "    break\n",
        "\n",
        "else: print('train_set test passed')\n",
        "\n",
        "for idx in range(len(test_loader)):\n",
        "\n",
        "  x, y = test_loader[idx]\n",
        "  if len(x) != len(y):\n",
        "    print('test_set test not passed')\n",
        "    break\n",
        "\n",
        "else: print('test_set test passed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7_mFDqOkcnW",
        "outputId": "cd9c0ef3-38e5-44e4-a4b9-d03f8ae6711d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_set test passed\n",
            "test_set test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since most of the computation won't be done with strings, the following function will create a Look Up Table (LUT) that transforms string tokens into ```int``` tokens. "
      ],
      "metadata": {
        "id": "nqp5OuN8YTwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tokens_lut(train_dataset) -> Dict:\n",
        "    '''\n",
        "    Input:\n",
        "        train_dataset: Training dataset. \n",
        "\n",
        "        Don't tokenize test_set as later on,\n",
        "        we will be considering out-of-vocabulary words as <unk> tokens.\n",
        "\n",
        "        NOTE: Tokens MUST be lowered (.lower()) before considering them. \n",
        "\n",
        "    Ouput:\n",
        "        LUT[Dict]: {\n",
        "        word1: 0,\n",
        "        word2: 1,\n",
        "            ...\n",
        "        wordn: n - 1\n",
        "        }\n",
        "\n",
        "    '''\n",
        "    token_to_id = {}\n",
        "    current_id = 0\n",
        "    \n",
        "    for i in range(len(train_dataset)):\n",
        "        for word, label in zip(*train_dataset[i]):\n",
        "            token = word.lower()  # Lowercase the token\n",
        "\n",
        "            if token not in token_to_id:\n",
        "                token_to_id[token] = current_id # Each new token is saved with the corresponding ID\n",
        "                current_id += 1\n",
        "\n",
        "    return token_to_id\n",
        "\n",
        "LUT = create_tokens_lut(train_loader)\n",
        "LUT"
      ],
      "metadata": {
        "id": "rt-8QpTzYx2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f28be1-85a9-4d8c-dd34-14c18005e8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dilluns': 0,\n",
              " 'a': 1,\n",
              " '5': 2,\n",
              " 'rebere': 3,\n",
              " 'de': 4,\n",
              " 'hyacinto': 5,\n",
              " 'boneu': 6,\n",
              " 'hortola': 7,\n",
              " 'bara': 8,\n",
              " 'fill': 9,\n",
              " 'juan': 10,\n",
              " 'parayre': 11,\n",
              " 'defunct': 12,\n",
              " 'y': 13,\n",
              " 'maria': 14,\n",
              " 'ab': 15,\n",
              " 'anna': 16,\n",
              " 'donsella': 17,\n",
              " 'filla': 18,\n",
              " 't': 19,\n",
              " 'cases': 20,\n",
              " 'pages': 21,\n",
              " 'peyrona': 22,\n",
              " 'dit': 23,\n",
              " 'dia': 24,\n",
              " 'bernat': 25,\n",
              " 'call': 26,\n",
              " 'st': 27,\n",
              " 'esteva': 28,\n",
              " 'palau': 29,\n",
              " 'tordera': 30,\n",
              " 'guille': 31,\n",
              " 'catherina': 32,\n",
              " 'guillem': 33,\n",
              " 'boix': 34,\n",
              " 'la': 35,\n",
              " 'mora': 36,\n",
              " 'bisbat': 37,\n",
              " 'vich': 38,\n",
              " 'margarida': 39,\n",
              " 'jua': 40,\n",
              " 'perega': 41,\n",
              " 'comissari': 42,\n",
              " 'real': 43,\n",
              " 'habitant': 44,\n",
              " 'en': 45,\n",
              " 'viudo': 46,\n",
              " 'gratia': 47,\n",
              " 'carbonell': 48,\n",
              " 'negociant': 49,\n",
              " 'defuncts': 50,\n",
              " 'dijous': 51,\n",
              " '10': 52,\n",
              " 'montserrat': 53,\n",
              " 'gibert': 54,\n",
              " 'treballador': 55,\n",
              " 'traginer': 56,\n",
              " 'mar': 57,\n",
              " 'janota': 58,\n",
              " 'magdalena': 59,\n",
              " 'sabater': 60,\n",
              " 'dissapte': 61,\n",
              " '26': 62,\n",
              " 'jaume': 63,\n",
              " '#': 64,\n",
              " 'oller': 65,\n",
              " 'andreu': 66,\n",
              " 'pa': 67,\n",
              " 'lomar': 68,\n",
              " 'eularia': 69,\n",
              " 'pere': 70,\n",
              " 'llobateres': 71,\n",
              " 'ripollet': 72,\n",
              " 'maryanna': 73,\n",
              " 'dimecres': 74,\n",
              " '13': 75,\n",
              " '#donare': 76,\n",
              " 'llicentia': 77,\n",
              " 'antoni': 78,\n",
              " 'dauban': 79,\n",
              " 'del': 80,\n",
              " 'regne': 81,\n",
              " 'frança': 82,\n",
              " 'lo': 83,\n",
              " 'prat': 84,\n",
              " 'saldonia': 85,\n",
              " 'miquel': 86,\n",
              " 'sola': 87,\n",
              " 'orista': 88,\n",
              " 'elisabeth': 89,\n",
              " 'defucts': 90,\n",
              " 'diumege': 91,\n",
              " '16': 92,\n",
              " 'claus': 93,\n",
              " 'frances': 94,\n",
              " 'spaser': 95,\n",
              " 'habitat': 96,\n",
              " 'llacuna': 97,\n",
              " 'mag': 98,\n",
              " 'dalena': 99,\n",
              " 'valenti': 100,\n",
              " 'colomer': 101,\n",
              " 'pierola': 102,\n",
              " 'mates': 103,\n",
              " 'dosella': 104,\n",
              " 'lloch': 105,\n",
              " 'gabriel': 106,\n",
              " 'janer': 107,\n",
              " 'aulesa': 108,\n",
              " 'defuncta': 109,\n",
              " 'juana': 110,\n",
              " 'bruguera': 111,\n",
              " 'tots': 112,\n",
              " 'dita': 113,\n",
              " 'parrochia': 114,\n",
              " 'reberem': 115,\n",
              " 'arenas': 116,\n",
              " 'artes': 117,\n",
              " 'an': 118,\n",
              " 'dreu': 119,\n",
              " 'palomar': 120,\n",
              " 'antich': 121,\n",
              " 'angela': 122,\n",
              " 'als': 123,\n",
              " 'vilar': 124,\n",
              " 'francesch': 125,\n",
              " 'regordosa': 126,\n",
              " 'passamaner': 127,\n",
              " 'sarria': 128,\n",
              " 'beneta': 129,\n",
              " 'viuda': 130,\n",
              " 'golfes': 131,\n",
              " '31': 132,\n",
              " 'baldiri': 133,\n",
              " 'livia': 134,\n",
              " 'cornet': 135,\n",
              " 'manresa': 136,\n",
              " 'salvadora': 137,\n",
              " 'llorens': 138,\n",
              " 'moliner': 139,\n",
              " 'fransa': 140,\n",
              " 'iscla': 141,\n",
              " 'clara': 142,\n",
              " 'serra': 143,\n",
              " 'gurb': 144,\n",
              " 'dimars': 145,\n",
              " '12': 146,\n",
              " 'petit': 147,\n",
              " 'olivella': 148,\n",
              " 'sitges': 149,\n",
              " 'magi': 150,\n",
              " 'oliver': 151,\n",
              " 'hierona': 152,\n",
              " 'marti': 153,\n",
              " 'forner': 154,\n",
              " 'çarreal': 155,\n",
              " 'archebisbat': 156,\n",
              " 'tar': 157,\n",
              " 'ragona': 158,\n",
              " 'figue': 159,\n",
              " 'res': 160,\n",
              " 'divendres': 161,\n",
              " '15': 162,\n",
              " 'pasqual': 163,\n",
              " 'corder': 164,\n",
              " 'caldes': 165,\n",
              " 'monbuy': 166,\n",
              " 'cathe': 167,\n",
              " 'rina': 168,\n",
              " 'camp': 169,\n",
              " 'antonia': 170,\n",
              " 'benell': 171,\n",
              " 'calsater': 172,\n",
              " 'agulla': 173,\n",
              " 'phelip': 174,\n",
              " 'ges': 175,\n",
              " 'serdanya': 176,\n",
              " 'urgell': 177,\n",
              " 'pau': 178,\n",
              " 'junqueres': 179,\n",
              " 'eli': 180,\n",
              " 'sabeth': 181,\n",
              " 'bosch': 182,\n",
              " 'llissa': 183,\n",
              " 'demunt': 184,\n",
              " 'juna': 185,\n",
              " '25': 186,\n",
              " 'ramon': 187,\n",
              " 'joan': 188,\n",
              " 'hieronyma': 189,\n",
              " 'paula': 190,\n",
              " 'delvi': 191,\n",
              " 'centelles': 192,\n",
              " 'violant': 193,\n",
              " 'hieronym': 194,\n",
              " 'alella': 195,\n",
              " 'pons': 196,\n",
              " 'joval': 197,\n",
              " 'solsona': 198,\n",
              " 'mori': 199,\n",
              " 'devant': 200,\n",
              " 'barthomeu': 201,\n",
              " 'lacumba': 202,\n",
              " 'pastisser': 203,\n",
              " 'davesa': 204,\n",
              " 'romeu': 205,\n",
              " 'vilaredona': 206,\n",
              " 'botiguer': 207,\n",
              " 'montserrada': 208,\n",
              " 'phi': 209,\n",
              " 'lippa': 210,\n",
              " 'gari': 211,\n",
              " 'matalafer': 212,\n",
              " 'vilanova': 213,\n",
              " 'speransa': 214,\n",
              " 'per': 215,\n",
              " 'mans': 216,\n",
              " 'sr': 217,\n",
              " 'canoge': 218,\n",
              " 'amell': 219,\n",
              " 'vicari': 220,\n",
              " 'general': 221,\n",
              " 'bertran': 222,\n",
              " 'baxador': 223,\n",
              " 'eleonor': 224,\n",
              " 'vinyals': 225,\n",
              " '18': 226,\n",
              " 'roig': 227,\n",
              " 'thomas': 228,\n",
              " 'torroella': 229,\n",
              " 'mariner': 230,\n",
              " 'rafols': 231,\n",
              " 'boter': 232,\n",
              " 'cubelles': 233,\n",
              " 'elienor': 234,\n",
              " 'crespi': 235,\n",
              " 'algutsir': 236,\n",
              " 'extraordimari': 237,\n",
              " 'die': 238,\n",
              " 'ambros': 239,\n",
              " 'samora': 240,\n",
              " 'gelida': 241,\n",
              " 'piloter': 242,\n",
              " 'barca': 243,\n",
              " 'papiol': 244,\n",
              " 'sylvestre': 245,\n",
              " 'valiellas': 246,\n",
              " 'bernabe': 247,\n",
              " 'riera': 248,\n",
              " 'granollers': 249,\n",
              " 'blanquer': 250,\n",
              " 'miquela': 251,\n",
              " 'marianna': 252,\n",
              " 'major': 253,\n",
              " 'sastre': 254,\n",
              " 'vilassar': 255,\n",
              " 'luys': 256,\n",
              " 'soler': 257,\n",
              " 'prats': 258,\n",
              " 'colom': 259,\n",
              " 'pens': 260,\n",
              " 'francisca': 261,\n",
              " 'busquets': 262,\n",
              " 'valldoreig': 263,\n",
              " 'ribes': 264,\n",
              " 'feliu': 265,\n",
              " 'llobregat': 266,\n",
              " '21': 267,\n",
              " 'carles': 268,\n",
              " 'coll': 269,\n",
              " 'marques': 270,\n",
              " 'fuster': 271,\n",
              " 'mataro': 272,\n",
              " 'guiteres': 273,\n",
              " 'carra': 274,\n",
              " 'ter': 275,\n",
              " 'faura': 276,\n",
              " 'badalona': 277,\n",
              " 'simeon': 278,\n",
              " 'roldos': 279,\n",
              " 'vivent': 280,\n",
              " 'masnou': 281,\n",
              " 'collsabadell': 282,\n",
              " 'mas': 283,\n",
              " 'argentona': 284,\n",
              " 'ferrer': 285,\n",
              " 'premia': 286,\n",
              " 'balthesar': 287,\n",
              " 'morera': 288,\n",
              " 'susanna': 289,\n",
              " '27': 290,\n",
              " 'mr': 291,\n",
              " 'raphel': 292,\n",
              " 'candaler': 293,\n",
              " 'cera': 294,\n",
              " 'mathiana': 295,\n",
              " 'arcis': 296,\n",
              " 'planes': 297,\n",
              " 'vedrier': 298,\n",
              " 'sperasa': 299,\n",
              " 'clos': 300,\n",
              " 'doctor': 301,\n",
              " 'drets': 302,\n",
              " 'ciuteda': 303,\n",
              " 'sra': 304,\n",
              " 'raphela': 305,\n",
              " 'burgues': 306,\n",
              " 'puigcerda': 307,\n",
              " 'torres': 308,\n",
              " 'ponsa': 309,\n",
              " 'vicens': 310,\n",
              " 'hortela': 311,\n",
              " 'montells': 312,\n",
              " 'franch': 313,\n",
              " 'corredor': 314,\n",
              " 'orella': 315,\n",
              " 'illa': 316,\n",
              " 'julita': 317,\n",
              " '24': 318,\n",
              " 'dequer': 319,\n",
              " 'basi': 320,\n",
              " 'sadorni': 321,\n",
              " '23': 322,\n",
              " 'aguilar': 323,\n",
              " 'eu': 324,\n",
              " 'laria': 325,\n",
              " 'boxo': 326,\n",
              " 'carrater': 327,\n",
              " 'arus': 328,\n",
              " 'sabadell': 329,\n",
              " 'dionysa': 330,\n",
              " 'do': 331,\n",
              " 'sella': 332,\n",
              " 'pujades': 333,\n",
              " 'vila': 334,\n",
              " '17': 335,\n",
              " 'alzina': 336,\n",
              " 'cardedeu': 337,\n",
              " 'joana': 338,\n",
              " 'joa': 339,\n",
              " 'gori': 340,\n",
              " 'vilamajor': 341,\n",
              " 'arnau': 342,\n",
              " 'locos': 343,\n",
              " 'cotxero': 344,\n",
              " 'serralta': 345,\n",
              " 'sala': 346,\n",
              " 'taya': 347,\n",
              " 'dels': 348,\n",
              " 'damut': 349,\n",
              " 'dits': 350,\n",
              " 'morato': 351,\n",
              " 'gerrer': 352,\n",
              " 'sal': 353,\n",
              " 'vador': 354,\n",
              " 'maryangela': 355,\n",
              " 'jorba': 356,\n",
              " 'manovelles': 357,\n",
              " 'collbato': 358,\n",
              " 'nadal': 359,\n",
              " 'elies': 360,\n",
              " 'texidor': 361,\n",
              " 'lli': 362,\n",
              " 'serinya': 363,\n",
              " 'çavall': 364,\n",
              " 'rexach': 365,\n",
              " 'ramo': 366,\n",
              " 'segui': 367,\n",
              " '30': 368,\n",
              " 'xiol': 369,\n",
              " 'animals': 370,\n",
              " 'fill#': 371,\n",
              " 'dorothea': 372,\n",
              " 'gili': 373,\n",
              " 'catha': 374,\n",
              " 'diumenge': 375,\n",
              " '22': 376,\n",
              " 'semoler': 377,\n",
              " 'mari': 378,\n",
              " 'balcells': 379,\n",
              " '11': 380,\n",
              " 'font': 381,\n",
              " 'terrassa': 382,\n",
              " 'marga': 383,\n",
              " 'rida': 384,\n",
              " 'narcis': 385,\n",
              " 'guitart': 386,\n",
              " 'janot': 387,\n",
              " 'vilardona': 388,\n",
              " 'bellsol': 389,\n",
              " 'vilaro': 390,\n",
              " 'dona': 391,\n",
              " 'comadara': 392,\n",
              " 'ça': 393,\n",
              " 'vall': 394,\n",
              " 'espalter': 395,\n",
              " '6': 396,\n",
              " 'figuerola': 397,\n",
              " 'bal': 398,\n",
              " 'thesar': 399,\n",
              " 'torrellas': 400,\n",
              " 'puig': 401,\n",
              " 'las': 402,\n",
              " 'fexes': 403,\n",
              " 'duran': 404,\n",
              " 'mollet': 405,\n",
              " '7': 406,\n",
              " 'alomar': 407,\n",
              " 'calafat': 408,\n",
              " 'casanoves': 409,\n",
              " 'pescador': 410,\n",
              " 'garriga': 411,\n",
              " 'cabrera': 412,\n",
              " 'piera': 413,\n",
              " 'tudo': 414,\n",
              " 'strada': 415,\n",
              " 'cirurgia': 416,\n",
              " 'perpinya': 417,\n",
              " 'notari': 418,\n",
              " 'parelada': 419,\n",
              " 'telas': 420,\n",
              " 'sebastia': 421,\n",
              " 'guixols': 422,\n",
              " 'girona': 423,\n",
              " 'elisa': 424,\n",
              " 'beth': 425,\n",
              " 'peller': 426,\n",
              " 'tarragona': 427,\n",
              " 'barrera': 428,\n",
              " 'castellbo': 429,\n",
              " 'simon': 430,\n",
              " 'pedros': 431,\n",
              " 'tudela': 432,\n",
              " 'joseph': 433,\n",
              " 'giralt': 434,\n",
              " 'hostaler': 435,\n",
              " 'tremoler': 436,\n",
              " 'vidal': 437,\n",
              " 'pas': 438,\n",
              " 'dols': 439,\n",
              " 'malet': 440,\n",
              " 'robi': 441,\n",
              " 'bis': 442,\n",
              " 'bat': 443,\n",
              " 'comes': 444,\n",
              " 'dorrius': 445,\n",
              " 'eufrahina': 446,\n",
              " 'portas': 447,\n",
              " 'feraut': 448,\n",
              " 'retorsedor': 449,\n",
              " 'llana': 450,\n",
              " 'jonchar': 451,\n",
              " 'çacamp': 452,\n",
              " 'rocha': 453,\n",
              " '28': 454,\n",
              " 'rubi': 455,\n",
              " 'stephania': 456,\n",
              " 'barcelo': 457,\n",
              " '29': 458,\n",
              " 'sayol': 459,\n",
              " 'marcia': 460,\n",
              " 'francesca': 461,\n",
              " 'pros': 462,\n",
              " 'costant': 463,\n",
              " 'agusti': 464,\n",
              " 'flassader': 465,\n",
              " 'vivint': 466,\n",
              " 'eula': 467,\n",
              " 'ria': 468,\n",
              " 'oriol': 469,\n",
              " 'tamarit': 470,\n",
              " 'donsell': 471,\n",
              " 'domiciliat': 472,\n",
              " 'perot': 473,\n",
              " 'raymunda': 474,\n",
              " 'vilagaya': 475,\n",
              " 'corbera': 476,\n",
              " 'isabel': 477,\n",
              " 'guardia': 478,\n",
              " 'argenter': 479,\n",
              " 'madrona': 480,\n",
              " 'martell': 481,\n",
              " 'garbellador': 482,\n",
              " 'ricas': 483,\n",
              " 'blanch': 484,\n",
              " 'camprodo': 485,\n",
              " 'llinyola': 486,\n",
              " 'domenja': 487,\n",
              " 'vilafranca': 488,\n",
              " 'panaders': 489,\n",
              " 'sesoliveres': 490,\n",
              " 'subirats': 491,\n",
              " 'badia': 492,\n",
              " 'iga': 493,\n",
              " 'solinyach': 494,\n",
              " 'sobirats': 495,\n",
              " 'ferriola': 496,\n",
              " 'estella': 497,\n",
              " 'carrovires': 498,\n",
              " 'trullas': 499,\n",
              " 'decavals': 500,\n",
              " 'marser': 501,\n",
              " 'onofre': 502,\n",
              " 'canyet': 503,\n",
              " 'daguer': 504,\n",
              " 'bus': 505,\n",
              " 'quets': 506,\n",
              " 'oliva': 507,\n",
              " 'vaquer': 508,\n",
              " 'jove': 509,\n",
              " 'mestre': 510,\n",
              " 'natural': 511,\n",
              " 'gim': 512,\n",
              " 'rabassa': 513,\n",
              " 'sescomes': 514,\n",
              " 'beneya': 515,\n",
              " 'adroguer': 516,\n",
              " 'habi': 517,\n",
              " 'tant': 518,\n",
              " 'agnes': 519,\n",
              " 'arbos': 520,\n",
              " 'assahonador': 521,\n",
              " 'barrater': 522,\n",
              " 'mariangela': 523,\n",
              " 'torrent': 524,\n",
              " 'ceu': 525,\n",
              " '4': 526,\n",
              " 'massana': 527,\n",
              " 'pontons': 528,\n",
              " 'mitja': 529,\n",
              " 'mercader': 530,\n",
              " 'contijoch': 531,\n",
              " 'comalada': 532,\n",
              " 'cilurgia': 533,\n",
              " '19': 534,\n",
              " 'salvador': 535,\n",
              " 'joli': 536,\n",
              " 'bellver': 537,\n",
              " 'lleyda': 538,\n",
              " 'aliana': 539,\n",
              " 'fetjo': 540,\n",
              " 'cugat': 541,\n",
              " 'vallers': 542,\n",
              " 'moner': 543,\n",
              " 'costa': 544,\n",
              " 'maryana': 545,\n",
              " 'seloni': 546,\n",
              " 'puigmari': 547,\n",
              " 'damia': 548,\n",
              " 'hostal': 549,\n",
              " 'rich': 550,\n",
              " 'vilalba': 551,\n",
              " 'sagimon': 552,\n",
              " 'ferran': 553,\n",
              " 'donare': 554,\n",
              " 'gratis': 555,\n",
              " 'dosseig': 556,\n",
              " 'pia': 557,\n",
              " 'elna': 558,\n",
              " '14': 559,\n",
              " 'benet': 560,\n",
              " 'ifern': 561,\n",
              " 'farrer': 562,\n",
              " 'bigues': 563,\n",
              " 'garida': 564,\n",
              " 'blancafort': 565,\n",
              " 'boger': 566,\n",
              " 'montmany': 567,\n",
              " 'figueres': 568,\n",
              " 'sinter': 569,\n",
              " 'praxedis': 570,\n",
              " 'colell': 571,\n",
              " 'franca': 572,\n",
              " 'calvell': 573,\n",
              " 'sta': 574,\n",
              " 'perpetua': 575,\n",
              " 'ribalta': 576,\n",
              " 'granera': 577,\n",
              " 'march': 578,\n",
              " 'lledo': 579,\n",
              " 'magina': 580,\n",
              " 'ricart': 581,\n",
              " 'matheu': 582,\n",
              " 'marcer': 583,\n",
              " 'bellmut': 584,\n",
              " 'antiga': 585,\n",
              " 'savina': 586,\n",
              " 'gualba': 587,\n",
              " 'mercer': 588,\n",
              " 'victoria': 589,\n",
              " '8': 590,\n",
              " 'mont': 591,\n",
              " 'serrat': 592,\n",
              " 'ninou': 593,\n",
              " 'candia': 594,\n",
              " 'miql': 595,\n",
              " 'riu': 596,\n",
              " 'billes': 597,\n",
              " 'verdaguer': 598,\n",
              " 'llampis': 599,\n",
              " 'llibo': 600,\n",
              " 'monica': 601,\n",
              " 'vels': 602,\n",
              " 'sans': 603,\n",
              " 'masmitja': 604,\n",
              " 'sparaguera': 605,\n",
              " 'cantarell': 606,\n",
              " 'axa': 607,\n",
              " 'barbera': 608,\n",
              " 'rossell': 609,\n",
              " 'arago': 610,\n",
              " 'funct': 611,\n",
              " 'cardalech': 612,\n",
              " 'olivelles': 613,\n",
              " 'campins': 614,\n",
              " 'nicolau': 615,\n",
              " 'sanahuja': 616,\n",
              " 'despi': 617,\n",
              " 'boria': 618,\n",
              " 'mot': 619,\n",
              " 'boy': 620,\n",
              " 'francina': 621,\n",
              " 'alfonso': 622,\n",
              " 'margarit': 623,\n",
              " 'masquefa': 624,\n",
              " 'llavaneres': 625,\n",
              " 'arenys': 626,\n",
              " 'barriga': 627,\n",
              " 'lloberes': 628,\n",
              " 'lloberas': 629,\n",
              " 'tort': 630,\n",
              " 'fontanills': 631,\n",
              " 'galceran': 632,\n",
              " 'bru': 633,\n",
              " 'medicina': 634,\n",
              " 'hierony': 635,\n",
              " 'vernet': 636,\n",
              " 'llorenço': 637,\n",
              " 'nots': 638,\n",
              " 'agustina': 639,\n",
              " 'arno': 640,\n",
              " 'llotge': 641,\n",
              " 'sistaller': 642,\n",
              " 'cardus': 643,\n",
              " 'terme': 644,\n",
              " 'tavarner': 645,\n",
              " 'fost': 646,\n",
              " 'calopa': 647,\n",
              " 'gervasi': 648,\n",
              " 'amor': 649,\n",
              " 'deu': 650,\n",
              " 'cot': 651,\n",
              " 'piquer': 652,\n",
              " 'paleja': 653,\n",
              " 'bodo': 654,\n",
              " 'blanquinador': 655,\n",
              " 'defuct': 656,\n",
              " 'damiana': 657,\n",
              " 'manaut': 658,\n",
              " 'oms': 659,\n",
              " 'martina': 660,\n",
              " 'te': 661,\n",
              " 'xidor': 662,\n",
              " 'callas': 663,\n",
              " 'terinya': 664,\n",
              " 'balmasseda': 665,\n",
              " 'ciutat': 666,\n",
              " 'pro': 667,\n",
              " 'vincia': 668,\n",
              " 'biscaya': 669,\n",
              " 'julia': 670,\n",
              " 'massot': 671,\n",
              " 'forners': 672,\n",
              " 'berga': 673,\n",
              " 'subira': 674,\n",
              " 'sparter': 675,\n",
              " 'hypolita': 676,\n",
              " 'benavent': 677,\n",
              " 'carreter': 678,\n",
              " 'ca': 679,\n",
              " 'therina': 680,\n",
              " 'copons': 681,\n",
              " '3': 682,\n",
              " 'isach': 683,\n",
              " 'totel': 684,\n",
              " 'stamper': 685,\n",
              " 'valentia': 686,\n",
              " 'miranda': 687,\n",
              " 'lucia': 688,\n",
              " 'peralta': 689,\n",
              " 'sacastiella': 690,\n",
              " 'bartres': 691,\n",
              " 'bosca': 692,\n",
              " 'valls': 693,\n",
              " 'solitar': 694,\n",
              " 'salmell': 695,\n",
              " 'terrasola': 696,\n",
              " 'guell': 697,\n",
              " 'monmell': 698,\n",
              " 'angel': 699,\n",
              " 'uguet': 700,\n",
              " 'marmellar': 701,\n",
              " 'vinyes': 702,\n",
              " 'ur': 703,\n",
              " 'sol': 704,\n",
              " 'giro': 705,\n",
              " 'marles': 706,\n",
              " 'ribot': 707,\n",
              " 'genis': 708,\n",
              " 'horta': 709,\n",
              " 'artigues': 710,\n",
              " 'mallorca': 711,\n",
              " 'gaspar': 712,\n",
              " 'serrador': 713,\n",
              " 'sona': 714,\n",
              " 'bernada': 715,\n",
              " 'caramany': 716,\n",
              " 'nar': 717,\n",
              " 'cis': 718,\n",
              " 'cabessa': 719,\n",
              " 'spara': 720,\n",
              " 'guera': 721,\n",
              " 'garau': 722,\n",
              " 'thomasa': 723,\n",
              " 'jover': 724,\n",
              " 'tremp': 725,\n",
              " 'bonet': 726,\n",
              " 'caputxer': 727,\n",
              " 'coloma': 728,\n",
              " 'alegret': 729,\n",
              " 'ginestar': 730,\n",
              " 'serraller': 731,\n",
              " 'roses': 732,\n",
              " 'climent': 733,\n",
              " 'plegamans': 734,\n",
              " 'polonia': 735,\n",
              " 'ebrera': 736,\n",
              " 'castanyer': 737,\n",
              " 'baster': 738,\n",
              " 'martorell': 739,\n",
              " 'gali': 740,\n",
              " 'jaue': 741,\n",
              " 'vendrell': 742,\n",
              " 'perez': 743,\n",
              " 'florentina': 744,\n",
              " 'ayala': 745,\n",
              " 'correu': 746,\n",
              " 'castellet': 747,\n",
              " 'nars': 748,\n",
              " 'lluis': 749,\n",
              " 'fraça': 750,\n",
              " 'campana': 751,\n",
              " 'ayter': 752,\n",
              " 'ramoneda': 753,\n",
              " 'carreras': 754,\n",
              " 'gava': 755,\n",
              " 'barru': 756,\n",
              " 'fet': 757,\n",
              " 'munt': 758,\n",
              " 'torrella': 759,\n",
              " 'estasia': 760,\n",
              " 'igualada': 761,\n",
              " 'rovira': 762,\n",
              " 'carreres': 763,\n",
              " 'balle': 764,\n",
              " 'pujes': 765,\n",
              " 'marquesa': 766,\n",
              " 'molner': 767,\n",
              " 'porter': 768,\n",
              " 'bordes': 769,\n",
              " 'calsas': 770,\n",
              " 'ros': 771,\n",
              " 'prohensals': 772,\n",
              " 'martres': 773,\n",
              " 'isern': 774,\n",
              " 'al': 775,\n",
              " 'pr': 776,\n",
              " 'raurell': 777,\n",
              " 'tegas': 778,\n",
              " 'ella': 779,\n",
              " 'genelose': 780,\n",
              " 'italia': 781,\n",
              " 'pintor': 782,\n",
              " 'febrer': 783,\n",
              " 'saba': 784,\n",
              " 'dell': 785,\n",
              " 'orelles': 786,\n",
              " 'cordas': 787,\n",
              " 'viola': 788,\n",
              " 'guerau': 789,\n",
              " 'bestias': 790,\n",
              " 'salamo': 791,\n",
              " 'claria': 792,\n",
              " 'clariana': 793,\n",
              " 'horts': 794,\n",
              " 'falgueres': 795,\n",
              " 'brions': 796,\n",
              " 'ursol': 797,\n",
              " 'ronsana': 798,\n",
              " 'palaudaries': 799,\n",
              " 'dalta': 800,\n",
              " '9': 801,\n",
              " 'moral': 802,\n",
              " 'bo': 803,\n",
              " 'blay': 804,\n",
              " 'pampinya': 805,\n",
              " 'alavall': 806,\n",
              " 'barba': 807,\n",
              " 'forti': 808,\n",
              " 'grimal': 809,\n",
              " 'co': 810,\n",
              " 'lomer': 811,\n",
              " 'andreua': 812,\n",
              " 'staper': 813,\n",
              " 'pellisser': 814,\n",
              " 'nacijs': 815,\n",
              " 'pallas': 816,\n",
              " 'leyda': 817,\n",
              " 'montalt': 818,\n",
              " 'bas': 819,\n",
              " 'taix': 820,\n",
              " 'corda': 821,\n",
              " 'morell': 822,\n",
              " 'droguer': 823,\n",
              " 'calella': 824,\n",
              " 'folch': 825,\n",
              " '20': 826,\n",
              " 'bravo': 827,\n",
              " 'tapiner': 828,\n",
              " 'diego': 829,\n",
              " 'domingo': 830,\n",
              " 'fornes': 831,\n",
              " 'assaonador': 832,\n",
              " 'habit': 833,\n",
              " 'suert': 834,\n",
              " 'hieronya': 835,\n",
              " 'verder': 836,\n",
              " 'elena': 837,\n",
              " 'gallissa': 838,\n",
              " 'bauras': 839,\n",
              " 'guasch': 840,\n",
              " 'territori': 841,\n",
              " 'fonollar': 842,\n",
              " 'gramanet': 843,\n",
              " 'steva': 844,\n",
              " 'fossar': 845,\n",
              " 'fonolla': 846,\n",
              " 'juqueres': 847,\n",
              " 'serda': 848,\n",
              " 'sendra': 849,\n",
              " 'magrinya': 850,\n",
              " 'requesens': 851,\n",
              " 'ne': 852,\n",
              " 'gociant': 853,\n",
              " 'valetia': 854,\n",
              " 'solanes': 855,\n",
              " 'sarmet': 856,\n",
              " 'maymo': 857,\n",
              " 'monistrol': 858,\n",
              " '2': 859,\n",
              " 'boveres': 860,\n",
              " 'scudaller': 861,\n",
              " 'cecilia': 862,\n",
              " 'llinas': 863,\n",
              " 'catala': 864,\n",
              " 'mossach': 865,\n",
              " 'ugo': 866,\n",
              " 'molin': 867,\n",
              " 'rey': 868,\n",
              " 'carder': 869,\n",
              " 'muntaner': 870,\n",
              " 'cardona': 871,\n",
              " 'micas': 872,\n",
              " 'molinderey': 873,\n",
              " 'florencia': 874,\n",
              " 'grau': 875,\n",
              " 'llicetia': 876,\n",
              " 'moya': 877,\n",
              " 'sovils': 878,\n",
              " 'pla': 879,\n",
              " 'sentis': 880,\n",
              " 'destarach': 881,\n",
              " 'escola': 882,\n",
              " 'pineda': 883,\n",
              " 'eufrasina': 884,\n",
              " 'jorda': 885,\n",
              " 'valentina': 886,\n",
              " 'balsareny': 887,\n",
              " 'castello': 888,\n",
              " 'balsa': 889,\n",
              " 'grano': 890,\n",
              " 'llers': 891,\n",
              " 'christophol': 892,\n",
              " 'organs': 893,\n",
              " 'devall': 894,\n",
              " 'besalu': 895,\n",
              " 'bassa': 896,\n",
              " 'roca': 897,\n",
              " 'menat': 898,\n",
              " 'oriach': 899,\n",
              " 'termens': 900,\n",
              " 'gallart': 901,\n",
              " 'molanta': 902,\n",
              " 'dexen': 903,\n",
              " 'moles': 904,\n",
              " 'bellsola': 905,\n",
              " 'mogoda': 906,\n",
              " 'armengol': 907,\n",
              " 'basso': 908,\n",
              " 'splugues': 909,\n",
              " 'esteve': 910,\n",
              " 'vilarodona': 911,\n",
              " 'senauja': 912,\n",
              " 'tisorer': 913,\n",
              " 'climet': 914,\n",
              " 'dardet': 915,\n",
              " 'guilleumas': 916,\n",
              " 'serdanyola': 917,\n",
              " 'mort': 918,\n",
              " 'hospital': 919,\n",
              " 'mauri': 920,\n",
              " 'geltru': 921,\n",
              " 'galcera': 922,\n",
              " 'urgelles': 923,\n",
              " 'pares': 924,\n",
              " 'hiero': 925,\n",
              " 'nyma': 926,\n",
              " 'ortells': 927,\n",
              " 'brunet': 928,\n",
              " 'palou': 929,\n",
              " 'reptor': 930,\n",
              " 'casansals': 931,\n",
              " 'monpeo': 932,\n",
              " 'pastor': 933,\n",
              " 'homellons': 934,\n",
              " 'po': 935,\n",
              " 'blet': 936,\n",
              " 'dardenya': 937,\n",
              " 'don': 938,\n",
              " 'bellmunt': 939,\n",
              " 'brasser': 940,\n",
              " 'masdovelles': 941,\n",
              " 'massaguera': 942,\n",
              " 'mathia': 943,\n",
              " 'massaguer': 944,\n",
              " 'nego': 945,\n",
              " 'ciant': 946,\n",
              " 'quadriu': 947,\n",
              " 'boxera': 948,\n",
              " 'llibrater': 949,\n",
              " 'basili': 950,\n",
              " 'alexandre': 951,\n",
              " 'pomes': 952,\n",
              " 'tona': 953,\n",
              " 'residencia': 954,\n",
              " 'tries': 955,\n",
              " 'cervello': 956,\n",
              " 'marxant': 957,\n",
              " 'palafolls': 958,\n",
              " 'marca': 959,\n",
              " 'tosinch': 960,\n",
              " 'scalfape': 961,\n",
              " 'bartho': 962,\n",
              " 'meu': 963,\n",
              " 'musich': 964,\n",
              " 'porta': 965,\n",
              " 'luysa': 966,\n",
              " 'rigarda': 967,\n",
              " 'ribafort': 968,\n",
              " 'alsina': 969,\n",
              " 'gorgui': 970,\n",
              " 'corro': 971,\n",
              " 'lloret': 972,\n",
              " 'pujol': 973,\n",
              " 'dionys': 974,\n",
              " 'homs': 975,\n",
              " 'orrius': 976,\n",
              " 'pou': 977,\n",
              " 'molines': 978,\n",
              " 'cervera': 979,\n",
              " 'rostoll': 980,\n",
              " 'extraordinari': 981,\n",
              " 'masramon': 982,\n",
              " 'tra': 983,\n",
              " 'giner': 984,\n",
              " 'miramdeu': 985,\n",
              " 'guerri': 986,\n",
              " 'martha': 987,\n",
              " 'llobet': 988,\n",
              " 'rocafort': 989,\n",
              " 'llo': 990,\n",
              " 'bet': 991,\n",
              " 'sunyol': 992,\n",
              " 'rigual': 993,\n",
              " 'tauder': 994,\n",
              " 'torello': 995,\n",
              " 'vallromanes': 996,\n",
              " 'vilardell': 997,\n",
              " 'defucta': 998,\n",
              " 'arquer': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_oov_words(LUT = LUT, test_set = test_loader):\n",
        "    # Out of vocabulary list\n",
        "    oov_words = []\n",
        "\n",
        "    for i in range(len(test_set)):\n",
        "        for word, label in zip(*test_set[i]):\n",
        "            token = word.lower()  # Lowercase the token\n",
        "\n",
        "            if token not in LUT:\n",
        "                oov_words.append(token) # Add the tokens of the test dataloader to the list if they don't belong to the LUT\n",
        "\n",
        "    return oov_words\n",
        "            \n",
        "print(list(check_oov_words()))\n"
      ],
      "metadata": {
        "id": "fxkJj8tDe5WA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b89ba6ae-7f36-4433-e847-ad32653a6375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['argemir', 'nyella', 'rianna', 'bonastra', 'angli', 'victo', 'theodora', 'payas', 'monllor', 'rotxe', 'moreno', 'islla', 'brasil', 'gusman', 'more', 'islla', 'sto', 'thome', 'habitants', 'melcior', 'bachs', 'pachs', 'plans', 'begas', 'rius', 'galeras', 'box', 'deseny', 'francesa', 'ortiz', 'faneca', 'faneca', 'sobrevila', 'sobrevila', 'cabrer', 'carantela', 'mso', 'tibau', 'monblanch', 'tibau', 'broquets', 'noguera', 'gassull', 'llondra', 'quart', 'pallissa', 'vivints', 'cabus', 'felis', 'buyra', 'scrivent', 'castigaleu', 'comptat', 'ribagossa', 'fontanilles', 'conteso', 'tatare', 'idrach', 'peramon', 'peramon', 'terre', 'arisart', 'arisart', 'faja', 'pinya', 'faliu', 'campanya', 'faliu', 'majol', 'majol', 'guardi#', 'debarca', 'constansa', 'llorenci', 'caxaler', 'llorenci', 'aguller', 'poses', 'miro', 'darder', 'darder', 'garces', 'imaginayre', 'juan#', 'villaro', 'vilademaser', 'muntells', 'muntells', 'sengermes', 'cebriana', 'tamuyell', 'cabaner', 'manader', 'manader', 'campprecios', 'busquet', 'crich', 'spa', 'raguera', 'valta', 'arevig', 'dimarts', 'berenguer', 'alaverni', 'conflent', 'glandina', 'fabrega', 'fabrega', 'fargayre', 'gatuellas', 'darder', 'comi', 'rosa', 'blanquart', 'dimas', 'tisser', 'mandri', 'mandri', 'corties', 'clavetayre', 'cani', 'jonqueres', 'munmany', 'tarafa', 'castellterçol', 'novell', 'novell', 'planta', 'llobre', 'gat', 'testa', 'testa', 'auger', 'ratx', 'ayguardenter', 'perris', 'celles', 'assensio', 'ge', 'febres', 'fabres', 'hor', 'tola', 'sitjar', 'macip', 'ritoreta', 'campderos', 'morros', 'masseres', 'masseres', 'honorat', 'luciana']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to batch computation of some modules, sequences must have constant length. As a common practice, we will create three new tokens ```<bos>``` and ```<eos>``` for the start and the end of a given sequence and ```<unk>``` for unkown tokens in the application (test) layer or 0 padding during the training. Manually add those tokens to the ```LUT```. \n",
        " \n",
        "\n",
        "Under those constraints, fill the corresponding functions that will post-process each batch. Feel free to code more post-processing functions if you need it.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7zHKqdlAUtly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LUT['<unk>'] = len(LUT) + 1\n",
        "LUT['<bos>'] = len(LUT) + 1\n",
        "LUT['<eos>'] = len(LUT) + 1"
      ],
      "metadata": {
        "id": "tnpVY5K7iJ9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following lines are commented because they represent an alternative approach to preprocessing the input data"
      ],
      "metadata": {
        "id": "Fq6Y8oK5LU7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adds special tokens <bos> (beginning of sequence) and <eos> (end of sequence) to the beginning and end of a list of tokens, respectively.\n",
        "\"\"\"\n",
        "def add_bos_eos(tokens: List[str]) -> List[str]:\n",
        "    return ['<bos>'] + tokens + ['<eos>']\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dJInoq4KJUab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90d739dc-48c0-48ff-8988-62bc4157b40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef add_bos_eos(tokens: List[str]) -> List[str]:\\n    return ['<bos>'] + tokens + ['<eos>']\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pads the input list of tokens to a specified max_length using a specified padding_token. \n",
        "#If the input list is shorter than max_length, it appends the padding tokens to the list;\n",
        "#if the input list is longer, it truncates the list to max_length.\n",
        "\"\"\"\n",
        "def pad_sequence(tokens: List[str], max_length: int, padding_token: str = '<unk>') -> List[str]:\n",
        "    if len(tokens) < max_length:\n",
        "        return tokens + [padding_token] * (max_length - len(tokens))\n",
        "    else:\n",
        "        return tokens[:max_length]\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6K6az5T6LpoJ",
        "outputId": "c3868169-187e-40ab-eca0-ada0cfa65444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef pad_sequence(tokens: List[str], max_length: int, padding_token: str = '<unk>') -> List[str]:\\n    if len(tokens) < max_length:\\n        return tokens + [padding_token] * (max_length - len(tokens))\\n    else:\\n        return tokens[:max_length]\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Processes a batch of data, applying the add_bos_eos and pad_sequence functions to each list of words and labels in the batch.\n",
        "\"\"\"\n",
        "def process_batch(batch: List[Tuple[List[str], List[str]]], max_length: int) -> List[Tuple[List[str], List[str]]]:\n",
        "    processed_batch = []\n",
        "\n",
        "    for words, labels in batch:\n",
        "        words = add_bos_eos(words)\n",
        "        words = pad_sequence(words, max_length)\n",
        "\n",
        "        labels = add_bos_eos(labels)\n",
        "        labels = pad_sequence(labels, max_length, padding_token='other')\n",
        "\n",
        "        processed_batch.append((words, labels))\n",
        "\n",
        "    return processed_batch\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "WHSvuVzdL02K",
        "outputId": "0285e080-4f48-4348-cab6-1e5e2683890c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef process_batch(batch: List[Tuple[List[str], List[str]]], max_length: int) -> List[Tuple[List[str], List[str]]]:\\n    processed_batch = []\\n\\n    for words, labels in batch:\\n        words = add_bos_eos(words)\\n        words = pad_sequence(words, max_length)\\n\\n        labels = add_bos_eos(labels)\\n        labels = pad_sequence(labels, max_length, padding_token='other')\\n\\n        processed_batch.append((words, labels))\\n\\n    return processed_batch\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\n",
        "def complete_seq(X) -> List[List]:\n",
        "  '''\n",
        "\n",
        "    Input: \n",
        "      X: A batch of N sequences [\n",
        "        [word1, ..., wordn],\n",
        "        [word1, ..., wordm]\n",
        "      ]\n",
        "\n",
        "    Output:\n",
        "      A batch of N sequences with MAX_SEQUENCE_LENGTH tokens.\n",
        "        - The starting token will always be <sos>\n",
        "        - The last 'real' token <eos>\n",
        "        - Tokens from <eos> until MAX_SEQUENCE_LENGTH will be <unk> as 0 padding.\n",
        "\n",
        "  '''\n",
        "  complete_seq = []\n",
        "  for seq in X:\n",
        "      seq = ['<bos>'] + seq + ['<eos>']\n",
        "      seq += ['<unk>'] * (MAX_SEQUENCE_LENGTH - len(seq))\n",
        "      complete_seq.append(seq[:MAX_SEQUENCE_LENGTH])\n",
        "  return complete_seq\n",
        "\n",
        "def post_process(X, functions = [complete_seq,]):\n",
        "  for f in functions: X = f(X)\n",
        "  return X"
      ],
      "metadata": {
        "id": "oj0cq0SGUsgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER - Baseline Approach\n",
        "\n",
        "The first approach we will try is based on computing the probabilities for each word in our training corpus. This means computing the most likely category for each word in the dictionary.\n",
        "\n",
        "Compute the test categories predictions and measure the performance for this simple model."
      ],
      "metadata": {
        "id": "GKuJPkJOanmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_emissions_dict(dataloader) -> Dict:\n",
        "  '''\n",
        "\n",
        "    Given the train loader ```dataloader```\n",
        "     this function will compute the max likelihood dictionary for each word.\n",
        "\n",
        "  Input:\n",
        "    dataloader: train loader with EsposallesTextDataset\n",
        "  \n",
        "  Outputs:\n",
        "    Dict: {\n",
        "      pagès: {name: X occupation: X}, # REMEMBER TO LOWER YOUR TOKENS!\n",
        "            ...\n",
        "      LUT - wordn: {category: x%, ...}\n",
        "    }\n",
        "\n",
        "  '''\n",
        "  emissions_dict = {}\n",
        "  \n",
        "  token_counts = Counter()\n",
        "  label_counts = Counter()\n",
        "  token_label_counts = Counter()\n",
        "  \n",
        "\n",
        "  for i in range(len(dataloader)):\n",
        "      for word,label in zip(*dataloader[i]):\n",
        "          token = word.lower()\n",
        "          token_counts[token] += 1\n",
        "          label_counts[label] += 1\n",
        "          token_label_counts[(token,label)] += 1\n",
        "  \n",
        "  for token, label_count in token_label_counts.items():\n",
        "    token, label = token\n",
        "    if token not in emissions_dict:\n",
        "        emissions_dict[token] = {}\n",
        "    emissions_dict[token][label] = label_count / token_counts[token]\n",
        "\n",
        "  return emissions_dict\n"
      ],
      "metadata": {
        "id": "rYA9D-twbego"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priors = compute_emissions_dict(train_loader)"
      ],
      "metadata": {
        "id": "uFQe-5VykF9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, as an example, your emissions dictionary should yield the following emission:\n",
        "\n",
        "$P(location |$ ```Prats``` $) = 18\\%$\n",
        "\n",
        "$P(surname |$ ```Prats``` $) = 72\\%$\n",
        "\n",
        "$P(other |$ ```Prats``` $) = 9\\%$"
      ],
      "metadata": {
        "id": "OozkEhtlQow-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "priors['prats']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEFUzlRpQmTa",
        "outputId": "a2301636-1504-4580-d6e9-3c288dcab0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'location': 0.18181818181818182,\n",
              " 'surname': 0.7272727272727273,\n",
              " 'other': 0.09090909090909091}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method has its limitations in terms of lack of context and, therefore, low expresivity. \n",
        "\n",
        "The following function will compute the confusion matrix for the predictions in the ```test_set``` in order to find the most problematic words. \n",
        "\n",
        "* What do they all have in common? \n",
        "* What kind of words are the least performers?\n",
        "* What's your solution for out-of-vocabulary words? Can you provide a prediction for those?"
      ],
      "metadata": {
        "id": "p1JaHhJvcwK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test_set(emissions, test_set):\n",
        "  \n",
        "  '''\n",
        "\n",
        "  s: casament eduard pages\n",
        "\n",
        "  prediccio: [None, nom, ofici]\n",
        "\n",
        "  Important: \n",
        "    Remember to check if you can provide a label for each word (OOVs?).\n",
        "    What's your solution for those you cannot classify? Justify.\n",
        "\n",
        "      tip: be as creative as you want.\n",
        "\n",
        "  '''\n",
        "  predictions = []\n",
        "  for i in range(len(test_set)):\n",
        "        seq_predictions = []\n",
        "        for word, label in zip(*test_set[i]):\n",
        "            token = word.lower()\n",
        "            if token in emissions:\n",
        "                pred_label = max(emissions[token], key=emissions[token].get)\n",
        "            else:\n",
        "                # Use the most frequent label for OOV words (simple but effective)\n",
        "                pred_label = 'other'\n",
        "            seq_predictions.append(pred_label)\n",
        "\n",
        "        predictions.append(seq_predictions)\n",
        "\n",
        "  return predictions\n",
        "\n",
        "\n",
        "predictions = predict_test_set(priors, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "lDQeZ1_Zcbqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_common_errors(x_test: List[List], y_pred: List[List], y_true: List[List]) -> Dict:\n",
        "    '''\n",
        "        Input: \n",
        "        x_test: A list with each sample in the corpus with the words for which we\n",
        "    ran each prediction\n",
        "            [\n",
        "            ['lorem', 'ipsum', 'dolor', 'sit', 'amet'],\n",
        "            ['Hello', 'world', '!!!'],\n",
        "            ]\n",
        "        \n",
        "        y_pred: A list with the predicted labels for each word in x_test corpus.\n",
        "            [\n",
        "            ['1', '0', '0', '1', '2'],\n",
        "            ['2', '1', '0'],\n",
        "            ]\n",
        "        \n",
        "        y_true: GT for the x_test sample\n",
        "            [\n",
        "            ['0', '0', '0', '1', '2'],\n",
        "            ['0', '1', '0'],\n",
        "            ]\n",
        "        {\n",
        "        pages: [{'pred': prediction, gt: label}, {'pred': prediction, 'gt': label}, ...]\n",
        "        }\n",
        "    '''\n",
        "    x_test_words = [word for List in x_test for word in List]\n",
        "    y_pred_labels = [label for List in y_pred for label in List]\n",
        "    y_true_labels = [label for List in y_true for label in List]\n",
        "\n",
        "    errors = {word: {\"pred\": pred, 'gt': gt} for word, pred, gt in zip(x_test_words, y_pred_labels, y_true_labels) if pred != gt}\n",
        "\n",
        "    return errors"
      ],
      "metadata": {
        "id": "SP_XzMBrgc-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following code cell is for testing"
      ],
      "metadata": {
        "id": "FkNcqciiojxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = [['lorem', 'ipsum', 'dolor', 'sit', 'amet'], ['Hello', 'world', '!!!']]\n",
        "y_pred = [['1', '0', '0', '1', '2'],['2', '1', '0']]\n",
        "y_true = [['0', '0', '0', '1', '2'],['0', '1', '0']]\n",
        "\n",
        "find_common_errors(x_test, y_pred, y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZi8NJEZoFzH",
        "outputId": "a03b8fee-9f64-410d-f51a-b3eeaab88100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lorem': {'pred': '1', 'gt': '0'}, 'Hello': {'pred': '1', 'gt': '2'}}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_token_precision(x_test: List[List], y_pred: List[List], y_true: List[List]):\n",
        "    errors = find_common_errors(x_test, y_pred, y_true)\n",
        "    x_test_words = [word for List in x_test for word in List]\n",
        "    error_rate = len(errors)/len(x_test_words)\n",
        "    return 1 - error_rate"
      ],
      "metadata": {
        "id": "nw3VbHuujeAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following code cell is for testing"
      ],
      "metadata": {
        "id": "5EF-Mv2fqMLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = [['lorem', 'ipsum', 'dolor', 'sit', 'amet'], ['Hello', 'world', '!!!']]\n",
        "y_pred = [['1', '0', '0', '1', '2'],['2', '1', '0']]\n",
        "y_true = [['0', '0', '0', '1', '2'],['0', '1', '0']]\n",
        "\n",
        "compute_token_precision(x_test, y_pred, y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30jeDApYptvV",
        "outputId": "08aa7b52-ac7d-4422-9990-7b65d1912dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_common_errors([test_loader[idx][0] for idx in range(len(test_loader))], predictions, [test_loader[idx][1] for idx in range(len(test_loader))])['Esteva']\n",
        "compute_token_precision([test_loader[idx][0] for idx in range(len(test_loader))], predictions, [test_loader[idx][1] for idx in range(len(test_loader))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-bjHuqWLH_w",
        "outputId": "d7d04bf3-f96b-4cdd-d2c1-a066f7167864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9308014161570647"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze the results for the common errors."
      ],
      "metadata": {
        "id": "wgH5jj9za5EH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "Here, write a brief conclusion for this notebook reffering to the main differences, advantages and disadvantages for each approach.\n"
      ],
      "metadata": {
        "id": "Qa8EqtjvisI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Your conclusions here\n",
        "\n",
        "After analyzing the given code, we can draw the following conclusions:\n",
        "\n",
        "The task at hand is Named Entity Recognition (NER) and sequence labeling in historical handwritten documents. The goal is to extract information from these documents and categorize the entities into semantic categories such as family names, places, occupations, etc.\n",
        "\n",
        "The code uses a simple baseline approach to predict the named entity categories, which involves computing the most likely category for each word based on the training data.\n",
        "\n",
        "The baseline approach does not take context into consideration and is limited in its expressiveness. It also faces challenges in handling out-of-vocabulary (OOV) words. The current solution for OOV words is to assign them the most frequent label, which may not be the best approach in all cases.\n",
        "\n",
        "The confusion matrix and common errors are computed to analyze the model's performance. The most problematic words are identified and analyzed for any common patterns or characteristics.\n",
        "\n",
        "Based on these conclusions, the baseline approach can be improved upon by incorporating context-aware methods or other techniques for handling OOV words. These improvements could potentially lead to better performance and more accurate predictions for the NER task in historical handwritten documents."
      ],
      "metadata": {
        "id": "Ak0r9CPEjEb0"
      }
    }
  ]
}